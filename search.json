[
  {
    "objectID": "Templates/DS350_Template.html",
    "href": "Templates/DS350_Template.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "TODO: Update with template from Paul\n\n\n\n\n Back to top"
  },
  {
    "objectID": "story_telling.html",
    "href": "story_telling.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "story_telling.html#title-2-header",
    "href": "story_telling.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "DS250 Projects",
    "section": "",
    "text": "Project 1\nProject 2\nProject 3\nProject 4\nProject 5",
    "crumbs": [
      "DS250 Projects"
    ]
  },
  {
    "objectID": "projects.html#repo-for-all-my-projects",
    "href": "projects.html#repo-for-all-my-projects",
    "title": "DS250 Projects",
    "section": "",
    "text": "Project 1\nProject 2\nProject 3\nProject 4\nProject 5",
    "crumbs": [
      "DS250 Projects"
    ]
  },
  {
    "objectID": "Projects/proj.4.writeup.3.2.24.html",
    "href": "Projects/proj.4.writeup.3.2.24.html",
    "title": "Client Report - Project 4: Can You Predict That?",
    "section": "",
    "text": "I built a model that predicts with about 93% accuracy whether a house was built before 1980 or not. In this report, you will find an analysis and evaluation of the correlation between different features of the data and the value of 'before1980'. Then a classification model is tuned and trained to make the desired prediction. A detailed description of the process is included. Then you will find an exploration of feature importance given by the model. Lastly, you will find an evaluation of the model’s performance, using several different metrics.\n\n\nRead and format project data\n# Include and execute your code here\n\nurl = r'https://raw.githubusercontent.com/byuidatascience/data4dwellings/master/data-raw/dwellings_ml/dwellings_ml.csv'\ndf = pd.read_csv(url)",
    "crumbs": [
      "DS250 Projects",
      "Project 4"
    ]
  },
  {
    "objectID": "Projects/proj.4.writeup.3.2.24.html#elevator-pitch",
    "href": "Projects/proj.4.writeup.3.2.24.html#elevator-pitch",
    "title": "Client Report - Project 4: Can You Predict That?",
    "section": "",
    "text": "I built a model that predicts with about 93% accuracy whether a house was built before 1980 or not. In this report, you will find an analysis and evaluation of the correlation between different features of the data and the value of 'before1980'. Then a classification model is tuned and trained to make the desired prediction. A detailed description of the process is included. Then you will find an exploration of feature importance given by the model. Lastly, you will find an evaluation of the model’s performance, using several different metrics.\n\n\nRead and format project data\n# Include and execute your code here\n\nurl = r'https://raw.githubusercontent.com/byuidatascience/data4dwellings/master/data-raw/dwellings_ml/dwellings_ml.csv'\ndf = pd.read_csv(url)",
    "crumbs": [
      "DS250 Projects",
      "Project 4"
    ]
  },
  {
    "objectID": "Projects/proj.4.writeup.3.2.24.html#questiontask-1",
    "href": "Projects/proj.4.writeup.3.2.24.html#questiontask-1",
    "title": "Client Report - Project 4: Can You Predict That?",
    "section": "Question|Task 1",
    "text": "Question|Task 1\nCreate 2-3 charts that evaluate potential relationships between the home variables and 'before1980'. Explain what you learn from the charts that could help a machine learning algorithm.\nI chose to use histograms because I am comparing two aggregate functions rather than a distribution of data; thus my options are limited.\n\nFirst Variable\nI thought that the condition of the house could serve as an indicator for the age of the house. I chose to evaluate those houses in worst condition 'condition_AVG' and best condition 'condition_Excel'. Of course, this is operating under the understanding I have that these are the extremes in the ‘condition’ categories.\n\n\nShow the code\n#group by whether house was built before 1980 and after 1980, sum condition of house\n\ncond = df.groupby('before1980')[['condition_AVG',\n       'condition_Excel', 'condition_Fair', 'condition_Good',\n       'condition_VGood']].sum().reset_index()\ncond['before1980'] = np.where(cond['before1980'] == 0 , 'After' , 'Before')\n\nfig = px.histogram(\n  cond,\n  x = 'before1980',\n  y = 'condition_AVG',\n  labels = {'before1980': 'Before 1980?' , 'condition_AVG': 'Total of Average Condition'},\n  color = 'condition_AVG',\n  title = 'Average Condition'\n).update_layout(\n  title_x = 0.5,\n  yaxis_title = 'Total of Average Condition'\n)\n\nfig2 = px.histogram(\n  cond,\n  x = 'before1980',\n  y = 'condition_Excel',\n  labels = {'before1980': 'Before 1980?' , 'condition_Excel': 'Total of Excellent Condition'},\n  color = 'condition_Excel',\n  title = 'Excellent Condition'\n).update_layout(\n  title_x = 0.5,\n  yaxis_title='Total of Excellent Condition'\n\n)\n\nfig.show() \nfig2.show()\n\n\n                                                \n\n\n                                                \n\n\nBased on the data, it appears that houses built before 1980, compared to houses built after 1980, are more frequently in average condition, and less frequently in excellent condition. This could potentially serve as an indicator for our machine learning model.\n\n\nVariables 2-4\nThe second set of variables I decided to compare was architcture styles. I reasoned that different styles might trend at different times.\n\n\nShow the code\n#creates style df\nstyle = df.groupby('before1980')[['arcstyle_BI-LEVEL', 'arcstyle_CONVERSIONS', 'arcstyle_END UNIT',\n       'arcstyle_MIDDLE UNIT', 'arcstyle_ONE AND HALF-STORY',\n       'arcstyle_ONE-STORY', 'arcstyle_SPLIT LEVEL', 'arcstyle_THREE-STORY',\n       'arcstyle_TRI-LEVEL', 'arcstyle_TRI-LEVEL WITH BASEMENT',\n       'arcstyle_TWO AND HALF-STORY', 'arcstyle_TWO-STORY']].sum().reset_index()\n\nstyle['before1980'] = np.where(style['before1980'] == 0 , 'After' , 'Before')\n\n#returns columns where yes &gt; no\n[name for name in style.columns if style.at[1 , name] &gt; style.at[0 , name]]\n\n#creates charts for most significant variables\nfor i in ['arcstyle_ONE AND HALF-STORY','arcstyle_ONE-STORY', 'arcstyle_CONVERSIONS']:\n  yval = i[9:] #identifier of column\n\n  fig = px.histogram(\n    style, \n    x = 'before1980',\n    y = i,\n    title = f'Architecture Style- {yval}',\n    color = 'before1980'\n  ).update_layout(\n    title_x = 0.5,\n    yaxis_title = yval,\n    \n  )\n\n  fig.show()\n\n\n                                                \n\n\n                                                \n\n\n                                                \n\n\nSeven styles are present more often in houses built before 1980, but the three types that show the largest disparity are 'arcstyle_CONVERSIONS', 'arcstyle_ONE AND HALF-STORY', and 'arcstyle_ONE-STORY'. It is conceivable that houses built before 1980 might be more likely to fit into one of these categories.\n\n\nCorrelation Calculations\nI created a correlation matrix to see how each feature variable relates to the target variable.\n\n\nShow the code\n#code taken from Slack\n\ncorrelation_matrix = df.drop(columns = ['parcel' , 'yrbuilt']).corr()\n# Get absolute values of correlation matrix\nabs_corr = correlation_matrix.abs()\n\n# Get the indices of the top n correlation values\nn = 15\n# Adjust as needed\ntop_corr_cols = abs_corr.nlargest(n, 'before1980')['before1980'].index\n\n#print corr matrix results\nprint(correlation_matrix.loc[top_corr_cols[1:], 'before1980'])\n\n# dict of top 15 correlation values\nc = dict(correlation_matrix.loc[top_corr_cols[1:], 'before1980'])\n\ncfig = px.bar(\n  x = list(c.keys()),\n  y= list(c.values()),\n  color = list(c.keys()),\n  title = \"Variables with Highest Correlation to 'before1980'\" \n).update_layout(\n  title_x = 0.5,\n  yaxis_title = 'Correlation'\n).update_xaxes(\n  tickvals = [],\n  ticktext = []\n  )\n\ncfig.show()\n\n\narcstyle_ONE-STORY    0.499333\nstories              -0.469182\nnumbaths             -0.437043\nquality_C             0.373930\narcstyle_TWO-STORY   -0.373308\ngartype_Att          -0.369099\nlivearea             -0.362384\nquality_B            -0.324754\nstatus_V             -0.303040\nstatus_I              0.303040\ngartype_Det           0.301321\ncondition_AVG        -0.288991\ncondition_Good        0.277334\ntasp                 -0.191230\nName: before1980, dtype: float64\n\n\n                                                \n\n\nSome of the variables that I previously compared do exhibit moderate correlation to the target variable, such as 'arcstyle_ONE-STORY', 'arcstyle_TWO-STORY', 'condition_AVG', and 'condition_Good'. To avoid issues with dimensionality, I could potentially choose to feed the model only the features with the highest correlation to the target variable. The downside to this is that the model might miss more subtle trends in the data that could be important for maximizing accuracy.",
    "crumbs": [
      "DS250 Projects",
      "Project 4"
    ]
  },
  {
    "objectID": "Projects/proj.4.writeup.3.2.24.html#questiontask-2",
    "href": "Projects/proj.4.writeup.3.2.24.html#questiontask-2",
    "title": "Client Report - Project 4: Can You Predict That?",
    "section": "Question|Task 2",
    "text": "Question|Task 2\nBuild a classification model labeling houses as being built “before 1980” or “during or after 1980”. Your goal is to reach or exceed 90% accuracy. Explain your final model choice (algorithm, tuning parameters, etc) and describe what other models you tried.\n\nExplanation\nI tried several different models, including GaussianNB() and DecisionTreeClassifier. However, I ended up rejecting both due to the accuracy being too low. I ultimately went with RandomForestClassifier(), due to its higher accuracy. I tried several different methods for finding tuning parameters, including utilizing hyperparameter tuning methods like GridSearchCV and RandomizedSearchCV. I spent some time researching the different hyperparameters available to tune in RandomForestClassifier(), and I picked several to try that seemed to be the most potentially impactful to me. The optimal parameters given by the different tuning methods I tried didn’t seem to result in more than a 1% increase in accuracy.\n\n\nStep 1\nThis code chunk picks the features and the target, and splits them into training, testing, x and y values. I dropped non-numeric variables, as well as the target variable 'before1980', and the variable 'yrbuilt' which was used to create the target variable. I picked the train-test split somewhat arbitrarily; there was little variation between different split ratios. I went with a 70/30 percent train-test ratio to allow for a higher amount of test data.\n\n\nShow the code\n#prep df\n#df.info()\n\ntarget = df['before1980']\n\nfeatures = df.drop(columns = ['before1980' , 'parcel' , 'yrbuilt'])\nfeatures.columns\n\n#features.info()\n\n#note that randomforest doesn't require scaling\n\n#try a 70-30 split\n\ntrain_data, test_data, train_targets, test_targets = train_test_split(features, target, test_size=.3)\n\n\n\n\nStep 2:\nWhile experimenting with my model, I decided to implement two different parameter tuning methods: GridSearchCV and RandomSearchCV. This helped me to find close-to-optimal parameters, and implemented cross-validation.\n\nExhaustive Grid Search\n\n\nShow the code\n#performs grid search\n\n#gets hyperaparameters for tuning\nRandomForestClassifier().get_params()\n\n#parameter grid to test\nparam_grid = {\n    'n_estimators': [100, 200, 300],  # Number of trees\n    'max_depth': [None, 5, 10, 15],    # Maximum tree depth\n    'min_samples_split': [2, 5, 10],    # Minimum number of samples required to split an internal node\n    'min_samples_leaf': [1, 2, 4],      # Minimum number of samples required to be at a leaf node\n    'max_features': ['sqrt', 'log2']   # Number of features to consider\n}\n\n#creates param grid search with cross-validation\ngrid = GridSearchCV(RandomForestClassifier(), \nparam_grid = param_grid, \nscoring = 'accuracy' , \nrandomstate = 42, \nverbose = 3, \nn_jobs = -1)\n\n#use consistent random state for consistent results\n\n#executes grid search\ngrid.fit(train_data, train_targets)\n\n#gets best params\nbest_params = grid.best_params_\nprint(\"Best Hyperparameters:\", best_params)\n\n#Best Hyperparameters: {'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 300}\n\n\n\nThe results of my exhaustive grid search display the following best parameters from my search: {'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 300}\n\nThe results of this search indicate that likely a further search is needed, given the best value for 'n_estimators' was the upper bound of the list of values I passed for that parameter , 300.\n\n\nRandom Search\n\n\nShow the code\n#performs random search\nparam_grid = {\n    'max_depth': randint(100, 1000),\n    'max_features': ['log2' , 'sqrt'],\n    'n_estimators': randint(100 , 1000),\n    'min_samples_split': randint(2, 20)\n}\n\nrandom_search = RandomizedSearchCV(\n    estimator=RandomForestClassifier(),\n    param_distributions=param_grid,\n    n_iter=100,  # Number of parameter settings that are sampled\n    cv=5, # num of folds\n    scoring='accuracy', #scoring metric\n    random_state=42,\n    verbose=3, #depth of real-time report\n    n_jobs=-1  # Use all available processing power\n)\n\n# Fit the model with the training data\nrandom_search.fit(train_data, train_targets)\n\n# Returns the best parameters when finished\nbest_params = random_search.best_params_\n\nprint(\"Best Hyperparameters:\", best_params)\n\n\n\nThe results of my random search were as follows: {'max_depth': 537, 'max_features': 'sqrt', 'min_samples_split': 3, 'n_estimators': 376}\n\nI decided to primarily use the results of the random search because of the lower time consumption, and having a larger distribution to try. I also felt that since it met and exceeded the goal of 90% accuracy, it was sufficient.\n\n\n\nStep 3: Final Model Training\nBelow is the final implementation of my model.\n\n\nShow the code\nmodel = None #to reset the model from previous fits\n\n#classifier = RandomForestClassifier() \n#default parameters results in 0.9239 average accuracy\n\nmodel = RandomForestClassifier(max_depth = 537, max_features = 'sqrt', n_estimators = 376 , min_samples_split = 3 , random_state = 42, n_jobs = -1)\n\n#splits data\ntrain_data, test_data, train_targets, test_targets = train_test_split(features, target, test_size=.3, random_state = 42)\n\n#train\nmodel.fit(train_data , train_targets)\n\n#predict\ntargets_predicted = model.predict(test_data)\n\n#compare\n\nnp1 = test_targets.to_numpy()\nnp2 = targets_predicted\n  \n#Calculates Evaluation Metrics\nacc = metrics.accuracy_score(np1 , np2)\nprec = metrics.precision_score(np1, np2)\nrec = metrics.recall_score(np1, np2)\nf1 = metrics.f1_score(np1, np2)\n\nprint('Accuracy:' , acc)\nprint('Precision:' , prec)\nprint('Recall:' , rec)\n\n\nAccuracy: 0.9294442828047716\nPrecision: 0.9378725355341586\nRecall: 0.9502903600464576\n\n\n\n\nFinal Results\nThe final average accuracy was 0.9294442828047716, or about 93%. This exceeds the required 90% accuracy.",
    "crumbs": [
      "DS250 Projects",
      "Project 4"
    ]
  },
  {
    "objectID": "Projects/proj.4.writeup.3.2.24.html#questiontask-3",
    "href": "Projects/proj.4.writeup.3.2.24.html#questiontask-3",
    "title": "Client Report - Project 4: Can You Predict That?",
    "section": "Question|Task 3",
    "text": "Question|Task 3\nJustify your classification model by discussing the most important features selected by your model. This discussion should include a chart and a description of the features.\nBelow is a chart displaying feature importance by feature.\n\n\nShow the code\nfeat_import = model.feature_importances_ #gets importance\nfi = dict(zip(features.columns, feat_import)) #assigns features to corresponding importance\nfi = dict(sorted(fi.items(), key=lambda item: item[1], reverse=True))#sorts highest to lowest\n\n#displays results\nfifig = px.bar(\n  x = list(fi.keys()),\n  y = list(fi.values()),\n  color = list(fi.values()),\n  title = 'Feature Importance by Feature'\n).update_layout(\n  title_x = 0.5,\n  xaxis_title = 'Feature',\n  yaxis_title = 'Importance'\n)\n\nfifig.show()\n\n\n                                                \n\n\nThe three feature with highest importance are 'livearea', 'arcstyle_ONE-STORY', and 'gartype_Att'. That means that the model determined that these are the most important features to use in making predictions. Their values hold more weight in the model’s decision-making. Many of the values with higher importance values also had higher correlation values in the previously-calculated correlation matrix.",
    "crumbs": [
      "DS250 Projects",
      "Project 4"
    ]
  },
  {
    "objectID": "Projects/proj.4.writeup.3.2.24.html#questiontask-4",
    "href": "Projects/proj.4.writeup.3.2.24.html#questiontask-4",
    "title": "Client Report - Project 4: Can You Predict That?",
    "section": "Question|Task 4",
    "text": "Question|Task 4\nDescribe the quality of your classification model using 2-3 different evaluation metrics. You also need to explain how to interpret each of the evaluation metrics you use.\nThe three other metrics I decided to use to evaluate the model were accuracy, a confusion matrix, and a classification report.\n\nConfusion Matrix\n\n\nShow the code\ncm = metrics.confusion_matrix(np1, np2)\nprint(\"Confusion Matrix:\\n\" , cm)\n\n\nConfusion Matrix:\n [[2298  271]\n [ 214 4091]]\n\n\n\nInterpretation\n\nTrue Negatives: (TN): 2298\nFalse Negatives: (FN): 214\nTrue Positives: (TP): 4091\nFalse Positives: (FP): 271\n\nOverall, we can see that the majority of the data was predicted correctly (TN + TP). Out of the data that was predicted incorrectly, false positives were more common than false negatives (FP &gt; FN). However, there are also far more total positive predictions than negative (TP + FP &gt; TN + FN). This matrix can be used to calculate certain ratios that will help to gain further insight into the data.\n\n\n\nClassification Report\n\n\nShow the code\nreport = metrics.classification_report(np1, np2)\nprint('Classification Report\\n' , report)\n\n\nClassification Report\n               precision    recall  f1-score   support\n\n           0       0.91      0.89      0.90      2569\n           1       0.94      0.95      0.94      4305\n\n    accuracy                           0.93      6874\n   macro avg       0.93      0.92      0.92      6874\nweighted avg       0.93      0.93      0.93      6874\n\n\n\n\nInterpretation\n\nPrecision Score: Out of all the times the model predicted a house built before 1980, it was correct about 91% of the time. Out of all the times the model predicted a house not built before 1980, it was right about 94% of the time. Overall, on average, the model was correct about 93% of the time it predicted true or false.\nRecall Score: Out of all of the house built before 1980 in the testing data, the model correctly predicted about 89% of the time. Out of all the houses that were not built before 1980, the model identified them about 95% of the time. Of all the samples that were true or false, the model correctly identified about 92% of them.\nF1 Score: Given an average f1 score of about 92%, it seems that the model does a decent job of balancing precision and recall. The model has high precision and recall, indicating good overall performance.\nSupport Each support value indicates how many samples were in each class. From this report we can see there were significantly more samples that were before 1980 than were not. This could potentially skew the results and affect the overall accuracy. Maybe this explains why the model was better at correctly classifying samples from before 1980 than it was at predicting samples that were not from before 1980.\n\n\n\n\nAccuracy\n\n\nShow the code\nprint('Accuracy Score:' , acc)\n\n\nAccuracy Score: 0.9294442828047716\n\n\nAccuracy was the main, defining scoring metric of the model. It was the metric used to find the best parameters, and it was the target involved in the goal of this project. The overall accuracy of the model was about 93%. This means that of all predictions made, the model correctly classified 93% of the samples.",
    "crumbs": [
      "DS250 Projects",
      "Project 4"
    ]
  },
  {
    "objectID": "Projects/proj.2.writeup.2.7.24.html",
    "href": "Projects/proj.2.writeup.2.7.24.html",
    "title": "Client Report - Project 2: Late flights and missing data (JSON files)",
    "section": "",
    "text": "I transformed the data to make it more useable. I analyzed the delays and determined that San Francisco has the worst (“highest”) proportion of delays. The best month to fly to avoid delays is September, although November is a close second. I added a column to the dataset that calculates the total number of weather delays. I performed analysis on the new weather column and determined that Atlanta has the worst weather delays, with Chicago being a close second.\n\n\nShow the code\n#this is my file path; I am not sure how the final html document works with reading files, but if I need to resubmit with different file path, please let me know.\n\ndf = pd.read_json(r'https://raw.githubusercontent.com/byuidatascience/data4missing/master/data-raw/flights_missing/flights_missing.json')",
    "crumbs": [
      "DS250 Projects",
      "Project 2"
    ]
  },
  {
    "objectID": "Projects/proj.2.writeup.2.7.24.html#elevator-pitch",
    "href": "Projects/proj.2.writeup.2.7.24.html#elevator-pitch",
    "title": "Client Report - Project 2: Late flights and missing data (JSON files)",
    "section": "",
    "text": "I transformed the data to make it more useable. I analyzed the delays and determined that San Francisco has the worst (“highest”) proportion of delays. The best month to fly to avoid delays is September, although November is a close second. I added a column to the dataset that calculates the total number of weather delays. I performed analysis on the new weather column and determined that Atlanta has the worst weather delays, with Chicago being a close second.\n\n\nShow the code\n#this is my file path; I am not sure how the final html document works with reading files, but if I need to resubmit with different file path, please let me know.\n\ndf = pd.read_json(r'https://raw.githubusercontent.com/byuidatascience/data4missing/master/data-raw/flights_missing/flights_missing.json')",
    "crumbs": [
      "DS250 Projects",
      "Project 2"
    ]
  },
  {
    "objectID": "Projects/proj.2.writeup.2.7.24.html#questiontask-1",
    "href": "Projects/proj.2.writeup.2.7.24.html#questiontask-1",
    "title": "Client Report - Project 2: Late flights and missing data (JSON files)",
    "section": "Question|Task 1",
    "text": "Question|Task 1\nFix all of the varied missing data types in the data to be consistent (all missing values should be displayed as “NaN”). In your report include one record example (one row) from your new data, in the raw JSON format. Your example should display the “NaN” for at least one missing value.\nThere are a number of steps to this process, because there are a number of things wrong with the dataset. Here are some things that I changed or fixed:\n\nNormalized datatypes to be consistent\nFilled in missing data with NaN for easier manipulation and analysis\nFilled in missing information where I could (i.e. missing airport codes)\nFixed typos to prevent future bugs\n\nHere is an example row of what the raw .json file looks like before my fix:\n\n\nShow the code\nrow_dict = df.iloc[0].to_dict()\nrow_dict #displays how row at index 0 will look in raw .json format\n\n\n{'airport_code': 'ATL',\n 'airport_name': 'Atlanta, GA: Hartsfield-Jackson Atlanta International',\n 'month': 'January',\n 'year': 2005.0,\n 'num_of_flights_total': 35048,\n 'num_of_delays_carrier': '1500+',\n 'num_of_delays_late_aircraft': -999,\n 'num_of_delays_nas': 4598,\n 'num_of_delays_security': 10,\n 'num_of_delays_weather': 448,\n 'num_of_delays_total': 8355,\n 'minutes_delayed_carrier': 116423.0,\n 'minutes_delayed_late_aircraft': 104415,\n 'minutes_delayed_nas': 207467.0,\n 'minutes_delayed_security': 297,\n 'minutes_delayed_weather': 36931,\n 'minutes_delayed_total': 465533}\n\n\n\nStep 1\nThe first fix I made was matching each airport code with an airport name, just for the sake of completeness in this dataset. All of the codes were present, but some of the names were missing (such as in the 3rd row):\n\n\nShow the code\ndf.filter(['airport_code', 'airport_name']).head(4)\n\n\n\n\n\n\n\n\n\n\nairport_code\nairport_name\n\n\n\n\n0\nATL\nAtlanta, GA: Hartsfield-Jackson Atlanta Intern...\n\n\n1\nDEN\nDenver, CO: Denver International\n\n\n2\nIAD\n\n\n\n3\nORD\nChicago, IL: Chicago O'Hare International\n\n\n\n\n\n\n\n\nMy fix was to update the airport_name column with the name of the airport that corresponds to the given code. Note the changes to line 3:\n\n\nShow the code\n#Exploration code:\n'''\ndf['airport_code'].unique()\n#there are no blank or invalid codes in airport_codes\ndf['airport_name'].unique()\n#there are blank names\n'''\n\n#gets rid of blank strings, ensures names are in proper places\n#ensures matching {code: name} values, with no unpaired values\nnames = [name for name in df['airport_name'].unique()]\nnames[2] = names[-1]\nnames.pop()\n\ncodes = [code for code in df['airport_code'].unique()]\n\n\n#creates dictionary with unique combinations of codes and names\n# dict(zip(unique codes, unique names))\ncode_name = \\\n  dict(zip(codes, names))\n\n\n#replaces all airport code values in df with appropriate value\n#and all names with appropriate names: for some reason\ndf['airport_name'] = df['airport_code'].replace(code_name)\n\ndf.filter(['airport_code', 'airport_name']).head(4)\n\n\n\n\n\n\n\n\n\n\nairport_code\nairport_name\n\n\n\n\n0\nATL\nAtlanta, GA: Hartsfield-Jackson Atlanta Intern...\n\n\n1\nDEN\nDenver, CO: Denver International\n\n\n2\nIAD\nWashington, DC: Washington Dulles International\n\n\n3\nORD\nChicago, IL: Chicago O'Hare International\n\n\n\n\n\n\n\n\n\n\nStep 2\nI checked to see if there were any fixes to be made to the months column: there were. February was spelled as “Febuary,” and I foresaw this causing problems in the future. I fixed the spelling error. There were also some blank values needing to be handled. For the sake of efficiency, I handle those later.\nNote the mistakes and “n/a” values:\n\n\nShow the code\n#This code chunk helps me to find invalid months\n\nmonths = ['January',\n          'February',\n          'March',\n          'April',\n          'May',\n          'June',\n          'July',\n          'August', \n          'September', \n          'October', \n          'November', \n          'December']\ninvalid_months = df['month'][~df['month'].isin(months)]\ninvalid_months.unique()\n\n#example of invalid months\ndf.filter(['month']).query('month == \"Febuary\" or month == \"n/a\"')[11:17]\n\n\n\n\n\n\n\n\n\n\nmonth\n\n\n\n\n95\nFebuary\n\n\n96\nFebuary\n\n\n97\nFebuary\n\n\n142\nn/a\n\n\n175\nFebuary\n\n\n176\nFebuary\n\n\n\n\n\n\n\n\nNote the fixes:\n\n\nShow the code\n#fixes months\nmonthfixes = {'Febuary': 'February',\n              'n/a': np.nan}\ndf['month'] = df['month'].replace(monthfixes)\n\n#displays fixed data\ndf.filter(['month']).query('month == \"February\" or month.isnull()')[11:17]\n\n\n\n\n\n\n\n\n\n\nmonth\n\n\n\n\n95\nFebruary\n\n\n96\nFebruary\n\n\n97\nFebruary\n\n\n142\nNaN\n\n\n175\nFebruary\n\n\n176\nFebruary\n\n\n\n\n\n\n\n\n\n\nStep 3\nI want to prepare to handle missing values, so I will assign placeholder values to each missing value. This just helps me as I normalize the data. Most notably, it helps me to see which values might be missing.\n\n\nShow the code\n#fills na values with -999 placeholder\ndf = df.fillna(-999)\n\n#df.count()\n\n\nAll values are now accounted for.\n\n\nStep 4\nThe column num_of_delays_carrier has an unexpected datatype object, so I must find the invalid values. From examining the dataset, I noticed some values contain the + character, Which appears to be the issue.\n\n\nShow the code\n#makes the change by getting rid of the '+', then attempts the type change; allows errors to be raised to detect other anomalies in the column\n\ndf['num_of_delays_carrier'] = df['num_of_delays_carrier'].str.replace('+', '').astype(np.float64, errors = 'raise')\n\n#df.info()\n\n\n\n\nStep 5\nI noticed that some of the columns that hold numeric values were int64 datatypes, which isn’t conducive to np.nan values. I decided to change these columns into float64 datatypes.\n\n\nShow the code\nfor column in df.columns[4:]:\n    df[column] = df[column].astype(np.float64, errors='raise')\n\n#successful\n#df.info()\n\n\n\n\nStep 6\nThe final step involves changing all placeholder values into useable np.nan values.\n\n\nShow the code\ndf = df.replace (-999, np.nan)\n\n#successful\n#df.count()\n\n\nHere is what the updated .json file should look like after my fixes:\n\n\nShow the code\nrow_dict = df.iloc[0].to_dict()\nrow_dict #displays how row at index 0 will look in raw .json format\n\n\n{'airport_code': 'ATL',\n 'airport_name': 'Atlanta, GA: Hartsfield-Jackson Atlanta International',\n 'month': 'January',\n 'year': 2005.0,\n 'num_of_flights_total': 35048.0,\n 'num_of_delays_carrier': 1500.0,\n 'num_of_delays_late_aircraft': nan,\n 'num_of_delays_nas': 4598.0,\n 'num_of_delays_security': 10.0,\n 'num_of_delays_weather': 448.0,\n 'num_of_delays_total': 8355.0,\n 'minutes_delayed_carrier': 116423.0,\n 'minutes_delayed_late_aircraft': 104415.0,\n 'minutes_delayed_nas': 207467.0,\n 'minutes_delayed_security': 297.0,\n 'minutes_delayed_weather': 36931.0,\n 'minutes_delayed_total': 465533.0}\n\n\nHere are some of the checks that I ran to ensure that my transformations were successful.\n\n\nShow the code\n#Run checks on the dataframe for unexpected values\n\ndf[~df['airport_code'].isin(codes)] #empty: successful\n\ndf[~df['airport_name'].isin(names)] #empty: successful\n\ndf[~df['month'].isin(months) & ~df['month'].isna()] #empty: successful\n\ndf[~df['year'].isin(range(int(df['year'].min()), int(df['year'].max() + 1))) & df['month'].isna()] #empty:successful\n\n\n\n\n\n\n\n\n\n\nairport_code\nairport_name\nmonth\nyear\nnum_of_flights_total\nnum_of_delays_carrier\nnum_of_delays_late_aircraft\nnum_of_delays_nas\nnum_of_delays_security\nnum_of_delays_weather\nnum_of_delays_total\nminutes_delayed_carrier\nminutes_delayed_late_aircraft\nminutes_delayed_nas\nminutes_delayed_security\nminutes_delayed_weather\nminutes_delayed_total",
    "crumbs": [
      "DS250 Projects",
      "Project 2"
    ]
  },
  {
    "objectID": "Projects/proj.2.writeup.2.7.24.html#questiontask-2",
    "href": "Projects/proj.2.writeup.2.7.24.html#questiontask-2",
    "title": "Client Report - Project 2: Late flights and missing data (JSON files)",
    "section": "Question|Task 2",
    "text": "Question|Task 2\nWhich airport has the worst delays? Discuss the metric you chose, and why you chose it to determine the “worst” airport. Your answer should include a summary table that lists (for each airport) the total number of flights, total number of delayed flights, proportion of delayed flights, and average delay time in hours.\nFor determining the “worst” airport with the “worst” delays, I decided to use the num_of_delays_total column. I chose to use the frequency metrics over the duration metrics because I feel that it is more relevant to the average traveler, although it is subjective. If I am walking into an airport, I personally want to know the likelihood that my flight will be delayed more than I want to know how long it will take if my flight is delayed.\n\nStep 1\nHere are the requested totals by airport:\n\n\nShow the code\ngrouped_df = df.groupby('airport_code')[[\n  'num_of_flights_total',\n  'num_of_delays_total',\n  ]].sum().sort_values('num_of_flights_total', ascending = False)\n\n#calculates percentage of delays, then turns into string with %\ngrouped_df['proportion_of_delays_percent'] = ((grouped_df['num_of_delays_total'] / grouped_df['num_of_flights_total'])*100).round(3).astype(str) + '%'\n\n#gets mean of every code type, then divides minutes by 60 to get hours\ngrouped_df['avg_delay_hours'] = ((df.groupby('airport_code')['minutes_delayed_total'].mean()) / (60)).round(3)\n\ngrouped_df = grouped_df.reset_index()\n\n# Set custom formatting for floating-point numbers: comma-delimited\npd.options.display.float_format = None\n\n\ngrouped_df\n\n\n\n\n\n\n\n\n\n\nairport_code\nnum_of_flights_total\nnum_of_delays_total\nproportion_of_delays_percent\navg_delay_hours\n\n\n\n\n0\nATL\n4430047.0\n902443.0\n20.371%\n6816.152\n\n\n1\nORD\n3597588.0\n830825.0\n23.094%\n7115.673\n\n\n2\nDEN\n2513974.0\n468519.0\n18.637%\n3178.457\n\n\n3\nSFO\n1630945.0\n425604.0\n26.096%\n3352.335\n\n\n4\nSLC\n1403384.0\n205160.0\n14.619%\n1278.203\n\n\n5\nSAN\n917862.0\n175132.0\n19.08%\n1044.981\n\n\n6\nIAD\n851571.0\n168467.0\n19.783%\n1298.419\n\n\n\n\n\n\n\n\n\n\nStep 2\nNow to examine the proportions of delays:\n\n\nShow the code\ngrouped_df.filter(['airport_code', 'proportion_of_delays_percent']).sort_values('proportion_of_delays_percent', ascending = False)\n\n\n\n\n\n\n\n\n\n\nairport_code\nproportion_of_delays_percent\n\n\n\n\n3\nSFO\n26.096%\n\n\n1\nORD\n23.094%\n\n\n0\nATL\n20.371%\n\n\n6\nIAD\n19.783%\n\n\n5\nSAN\n19.08%\n\n\n2\nDEN\n18.637%\n\n\n4\nSLC\n14.619%\n\n\n\n\n\n\n\n\n\n\nAnalysis\nWith just over 26% of flights being delayed at San Francisco airport, by this metric the San Francisco airport would be considered the airport with the worst delays.\n\n\nStep 3 (Extra Analysis)\nIt interested me to also explore other definitions of the “worst” delays. I decided to analyze the amount of delays caused by security issues. Security issues are arguably the worst kind of delay because of the potential threat to personal safety.\n\n\nShow the code\ngroup_df2 = df.groupby('airport_code')[[\n  'num_of_delays_total',\n  'num_of_delays_security'\n]].sum().reset_index().sort_values('num_of_delays_security', ascending = False)\n\ngroup_df2['security_delay_proportion'] = ((group_df2['num_of_delays_security'] / group_df2['num_of_delays_total']\n)*100).round(2).astype(str) + '%'\ngroup_df2.filter(['airport_code', 'security_delay_proportion']).sort_values('security_delay_proportion', ascending = False)\n\n\n\n\n\n\n\n\n\n\nairport_code\nsecurity_delay_proportion\n\n\n\n\n6\nSLC\n0.42%\n\n\n4\nSAN\n0.28%\n\n\n1\nDEN\n0.21%\n\n\n5\nSFO\n0.16%\n\n\n2\nIAD\n0.16%\n\n\n3\nORD\n0.1%\n\n\n0\nATL\n0.09%\n\n\n\n\n\n\n\n\nBy this metric, Salt Lake City has the “worst” delays.",
    "crumbs": [
      "DS250 Projects",
      "Project 2"
    ]
  },
  {
    "objectID": "Projects/proj.2.writeup.2.7.24.html#questiontask-3",
    "href": "Projects/proj.2.writeup.2.7.24.html#questiontask-3",
    "title": "Client Report - Project 2: Late flights and missing data (JSON files)",
    "section": "Question|Task 3",
    "text": "Question|Task 3\nWhat is the best month to fly if you want to avoid delays of any length?\n\nStep 1\nIf the total number of delays are grouped by month, we can see how many delays there were per month:\n\n\nShow the code\nmonth_df = df.groupby('month')[['num_of_delays_total', 'num_of_flights_total']].sum().reset_index()\n\n#formats it as a string, but changes back later to perform operations\nmonth_df['percent_delayed'] = ((month_df['num_of_delays_total'] / month_df['num_of_flights_total']) * 100).round(2).astype(str) + '%'\n\nmonth_df = month_df.dropna(subset=['month'])\n\nmonth_df\n\n\n\n\n\n\n\n\n\n\nmonth\nnum_of_delays_total\nnum_of_flights_total\npercent_delayed\n\n\n\n\n0\nApril\n231408.0\n1259723.0\n18.37%\n\n\n1\nAugust\n279699.0\n1335158.0\n20.95%\n\n\n2\nDecember\n303133.0\n1180278.0\n25.68%\n\n\n3\nFebruary\n248033.0\n1115814.0\n22.23%\n\n\n4\nJanuary\n265001.0\n1193018.0\n22.21%\n\n\n5\nJuly\n319960.0\n1371741.0\n23.33%\n\n\n6\nJune\n317895.0\n1305663.0\n24.35%\n\n\n7\nMarch\n250142.0\n1213370.0\n20.62%\n\n\n8\nMay\n233494.0\n1227795.0\n19.02%\n\n\n9\nNovember\n197768.0\n1185434.0\n16.68%\n\n\n10\nOctober\n235166.0\n1301612.0\n18.07%\n\n\n11\nSeptember\n201905.0\n1227208.0\n16.45%\n\n\n\n\n\n\n\n\n\n\nAnalysis\n\n\nShow the code\n#changes column back to float\nmonth_df['percent_delayed'] = month_df['percent_delayed'].str.rstrip('%').astype(float)\n\n#makes list of colors to differentiate minimum from the rest of the data\ncolors = list(np.where(\n  month_df['percent_delayed'] == month_df['percent_delayed'].min(),\n  'red',\n  'blue'\n))\n\ndelay_fig = px.bar(\n  month_df,\n  x = 'month',\n  y = 'percent_delayed',\n  labels = {'month': 'Month', 'percent_delayed': 'Percentage of Delayed Flights'},\n  title = 'Percentage of Delayed Flights by Month',\n  category_orders={'month': months}, #sorts chronologically\n  color = colors\n).update_layout(title_x = 0.5)\n\ndelay_fig.show()\n\n\n                                                \n\n\nIt appears that the fewest delays, as well as the lowest percentage of flights that are delayed, occur in September and November; thus the best months to fly to avoid delays are those. September has slightly more flights and slightly more delays, but a slightly lower proportion of delayed flights than November.",
    "crumbs": [
      "DS250 Projects",
      "Project 2"
    ]
  },
  {
    "objectID": "Projects/proj.2.writeup.2.7.24.html#questiontask-4",
    "href": "Projects/proj.2.writeup.2.7.24.html#questiontask-4",
    "title": "Client Report - Project 2: Late flights and missing data (JSON files)",
    "section": "Question|Task 4",
    "text": "Question|Task 4\nAccording to the BTS website, the “Weather” category only accounts for severe weather delays. Mild weather delays are not counted in the “Weather” category, but are actually included in both the “NAS” and “Late-Arriving Aircraft” categories. Your job is to create a new column that calculates the total number of flights delayed by weather (both severe and mild). You will need to replace all the missing values in the Late Aircraft variable with the mean. Show your work by printing the first 5 rows of data in a table. Use these three rules for your calculations:\nUse these three rules for your calculations:\n\n100% of delayed flights in the Weather category are due to weather\n30% of all delayed flights in the Late-Arriving category are due to weather.\nFrom April to August, 40% of delayed flights in the NAS category are due to weather. The rest of the months, the proportion rises to 65%.\n\nWith the given rules, I made the appropriate calculations and created a new column. For these calculations, to handle NaN values, I decided to drop any values where there were NaN values for month to avoid biasing the data. The dataset is still sufficiently large without these rows. I replaced any NaN values for num_of_delays_late_aircraft.\n\n\nShow the code\n'''Parameters:\n1 * 'num_of_delays_weather'\n0.3 * 'num_of_delays_late_aircraft'\nApril to August, 0.4 * 'num_of_delays_nas'\nSeptember to March, 0.65 * 'num_of_delays_nas'\n'''\n#check for nan values\ndf[df['num_of_delays_late_aircraft'].isna()]\n\n#calculates mean\nmean = df['num_of_delays_late_aircraft'].mean()\n\n#replaces NaN values in 'num_of_delays_late_aircraft' with mean\ndf['num_of_delays_late_aircraft'] = np.where(\n  df['num_of_delays_late_aircraft'].isna(),\n  mean,\n  df['num_of_delays_late_aircraft']\n)\n\n#check: successful\ndf[df['num_of_delays_late_aircraft'].isna()]\n\n#empty\ndf[df['num_of_delays_nas'].isna() & ~(df['num_of_delays_weather'] &gt;= 0)]\ndf[df['num_of_delays_weather'].isna() & ~(df['num_of_delays_weather'] &gt;= 0)]\n\n#in order to work with the dataset, I need to change NaN values to 0, but I must avoid conflicts with values that are truly zero when trying to convert the data back to NaN. So I make and apply changes to a dummy df\n\n#drops columns with no data for month to avoid biasing the data\ndummy_df = df.copy().dropna(subset=['month'])\n\n#categories of months\nsummermonths = ['April', 'May', 'June', 'July', 'August']\nwintermonths = ['January', 'February', 'March', 'September', 'October', 'November', 'December']\n\n# creates new column with given calculations\ndf['total_weather_delays'] = dummy_df['num_of_delays_weather'] + \\\n  (0.3 * dummy_df['num_of_delays_late_aircraft'])  + \\\n    np.where(\n      dummy_df['month'].isin(summermonths), \n      dummy_df['num_of_delays_nas'] * 0.4, \n      dummy_df['num_of_delays_nas'] * 0.65\n      ) \n\n\nHere is an example of the new data for the new column, total_weather_delays.\n\n\nShow the code\n#displays new column\ndf.filter(['total_weather_delays']).round(2).head(6)\n\n\n\n\n\n\n\n\n\n\ntotal_weather_delays\n\n\n\n\n0\n3769.43\n\n\n1\n1119.15\n\n\n2\n960.15\n\n\n3\n4502.25\n\n\n4\n674.70\n\n\n5\n1091.80",
    "crumbs": [
      "DS250 Projects",
      "Project 2"
    ]
  },
  {
    "objectID": "Projects/proj.2.writeup.2.7.24.html#questiontask-5",
    "href": "Projects/proj.2.writeup.2.7.24.html#questiontask-5",
    "title": "Client Report - Project 2: Late flights and missing data (JSON files)",
    "section": "Question|Task 5",
    "text": "Question|Task 5\nUsing the new weather variable calculated above, create a barplot showing the proportion of all flights that are delayed by weather at each airport. Discuss what you learn from this graph.\n\nStep 1\nI grouped the sum of the values of total_weather_delays by each airport, then plotted them.\n\n\nShow the code\n#sums total weather delays grouped by airport\ngrouped_airport = df.groupby('airport_code')['total_weather_delays'].sum().reset_index().sort_values(['total_weather_delays'])\n\n#creates bar graph of the data\nfig = px.bar(\n  grouped_airport,\n  x = 'airport_code',\n  y = 'total_weather_delays',\n  labels = {'airport_code':'Airport Code', 'total_weather_delays':'Total Weather Delays'},\n  title = 'Total Weather Delays By Airport'\n).update_layout(\n  title_x = 0.5\n)\n\nfig.show()\n\n\n                                                \n\n\n\n\nAnalysis\nBased on this data, It appears that Atlanta and Chicago have the worst airport delays due to weather. I would speculate that of the list of airports present in the data, Atlanta deals with the most hurricanes. Chicago likely deals with a lot of blizzards.\nWith more mild weather on average, it is not so surprising that San Diego and Dulles have the fewest weather delays.\nIt surprised me that San Francisco experiences more weather delays than Denver. Without doing further research, it’s difficult to understand why this is, but my guess is that Denver might be more prepared to handle weather delays than San Francisco.",
    "crumbs": [
      "DS250 Projects",
      "Project 2"
    ]
  },
  {
    "objectID": "ml.html",
    "href": "ml.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "ml.html#title-2-header",
    "href": "ml.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "full_stack.html",
    "href": "full_stack.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "full_stack.html#title-2-header",
    "href": "full_stack.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "competition.html",
    "href": "competition.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics\n#’ — #’ title: Palmer Penguins #’ author: Norah Jones #’ date: 3/12/23 #’ format: html #’ —\nlibrary(palmerpenguins)\n#’ ## Exploring the data #’ See ?@fig-bill-sizes for an exploration of bill sizes by species.\n#| label: fig-bill-sizes #| fig-cap: Bill Sizes by Species #| warning: false library(ggplot2) ggplot(data = penguins, aes(x = bill_length_mm, y = bill_depth_mm, group = species)) + geom_point(aes(color = species, shape = species), size = 3, alpha = 0.8) + labs(title = “Penguin bill dimensions”, subtitle = “Bill length and depth for Adelie, Chinstrap and Gentoo Penguins at Palmer Station LTER”, x = “Bill length (mm)”, y = “Bill depth (mm)”, color = “Penguin species”, shape = “Penguin species”)"
  },
  {
    "objectID": "competition.html#title-2-header",
    "href": "competition.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics\n#’ — #’ title: Palmer Penguins #’ author: Norah Jones #’ date: 3/12/23 #’ format: html #’ —\nlibrary(palmerpenguins)\n#’ ## Exploring the data #’ See ?@fig-bill-sizes for an exploration of bill sizes by species.\n#| label: fig-bill-sizes #| fig-cap: Bill Sizes by Species #| warning: false library(ggplot2) ggplot(data = penguins, aes(x = bill_length_mm, y = bill_depth_mm, group = species)) + geom_point(aes(color = species, shape = species), size = 3, alpha = 0.8) + labs(title = “Penguin bill dimensions”, subtitle = “Bill length and depth for Adelie, Chinstrap and Gentoo Penguins at Palmer Station LTER”, x = “Bill length (mm)”, y = “Bill depth (mm)”, color = “Penguin species”, shape = “Penguin species”)"
  },
  {
    "objectID": "cleansing.html",
    "href": "cleansing.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "cleansing.html#title-2-header",
    "href": "cleansing.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "exploration.html",
    "href": "exploration.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "exploration.html#title-2-header",
    "href": "exploration.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About Me",
    "section": "",
    "text": "I am 21 years old, and I am a first-year college student.\n\n\nI am intrigued by the world of data, and I am anxious to learn as much as I can about how to work with data to make inferences and decisions about the world around me. This portfolio should reflect my interest."
  },
  {
    "objectID": "index.html#data-science-student",
    "href": "index.html#data-science-student",
    "title": "About Me",
    "section": "",
    "text": "I am intrigued by the world of data, and I am anxious to learn as much as I can about how to work with data to make inferences and decisions about the world around me. This portfolio should reflect my interest."
  },
  {
    "objectID": "Projects/proj.1.writeup.1.19.23.html",
    "href": "Projects/proj.1.writeup.1.19.23.html",
    "title": "Client Report - Project 1: Names",
    "section": "",
    "text": "The popularity of my name, Andrew, was on the decline at the time of my birth. You are most likely to meet someone named Brittany around the age of 32. The names Peter, Paul, Martha, and Mary have all seen similar trends in popularity over the years, with some notable differences. The popularity of the names Marty and Martin seem to have been somewhat affected by the movie “Back to the Future,” as I originally hypothesized.",
    "crumbs": [
      "DS250 Projects",
      "Project 1"
    ]
  },
  {
    "objectID": "Projects/proj.1.writeup.1.19.23.html#elevator-pitch",
    "href": "Projects/proj.1.writeup.1.19.23.html#elevator-pitch",
    "title": "Client Report - Project 1: Names",
    "section": "",
    "text": "The popularity of my name, Andrew, was on the decline at the time of my birth. You are most likely to meet someone named Brittany around the age of 32. The names Peter, Paul, Martha, and Mary have all seen similar trends in popularity over the years, with some notable differences. The popularity of the names Marty and Martin seem to have been somewhat affected by the movie “Back to the Future,” as I originally hypothesized.",
    "crumbs": [
      "DS250 Projects",
      "Project 1"
    ]
  },
  {
    "objectID": "Projects/proj.1.writeup.1.19.23.html#questiontask-1",
    "href": "Projects/proj.1.writeup.1.19.23.html#questiontask-1",
    "title": "Client Report - Project 1: Names",
    "section": "Question|Task 1",
    "text": "Question|Task 1\nHow does your name at your birth year compare to its use historically?\n\nWorking with the Data\nThe first step is loading and graphing the data for my name, Andrew. I created a new dataframe with the relevant data using the .query() function. Then I created a simple line plot showing the usage of of “Andrew” historically, with some visual indicators for the data point located at the year of my birth, 2003.\n\n\nShow the code\n#builds new dataframe with instances of \"Andrew\"\nandrew_df = df.query('name == \"Andrew\"')\n\n#makes graph; x is year, y is total instances\nfig = px.line(\n    andrew_df,\n    x = 'year',\n    y = 'Total',\n    title = 'Historical Use of \"Andrew\"',\n    labels = {'year': 'Year'}\n).update_traces(\n    line_color = 'black')\n\nfig.add_vline(x = 2003, line_dash = 'dash', line_color = 'green')#my birthyear\nfig.add_hline(y = 20421, line_dash = 'dash', line_color = 'red')#total instances for my birth year\n\nfig.update_layout(\n    title_x = 0.5)\n\nfig.show()\n\n\n                                                \n\n\n\n\nAnalysis\nAt the time of my birth, the name Andrew was certainly more popular than the majority of other years. However, my birth occurs several years after Andrew’s peak popularity in 1990.\nHowever, its popularity wasn’t a direct influence in my parent’s decision to name me Andrew. I share a name with my father. So out of curiosity, I decided to compare how the popularity of the name during his birth year, 1977, measures up (displayed in purple):\n\n\nShow the code\nfig.add_vline(x = 1977, line_dash = 'dash', line_color = 'purple')#my father's birth year\n\nfig.show()\n\n\n                                                \n\n\nInterestingly, the name was less popular during the birth year of my father, and it was 13 years before its peak popularity.",
    "crumbs": [
      "DS250 Projects",
      "Project 1"
    ]
  },
  {
    "objectID": "Projects/proj.1.writeup.1.19.23.html#questiontask-2",
    "href": "Projects/proj.1.writeup.1.19.23.html#questiontask-2",
    "title": "Client Report - Project 1: Names",
    "section": "Question|Task 2",
    "text": "Question|Task 2\nIf you talked to someone named Brittany on the phone, what is your guess of his or her age? What ages would you not guess?\n\nWorking with the Data\nThe first step in answering this question is pulling the data for the name Brittany and displaying it visually with a line graph. I added a column of data representing real ages rather than years for relevancy.\n\n\nShow the code\nbrittany_df = df.query('name == \"Brittany\"')\nnew_index = [i for i in range(0, len(brittany_df))]\nbrittany_df = brittany_df.set_index(pd.Index(new_index))#sets a new 0-based index for easier referencing to replace index from previous df\n\nbrittany_df['true_ages'] = [2024-brittany_df.at[i, 'year'] for i in range(len(brittany_df))]\n#this creates a new column in the dataframe to allow easier, more relevant data to the question at hand;\n#I wrote this code using list comprehension before we learned about the .apply() function in class\n\nbrit_fig = px.line( #creates line plot\n    brittany_df,\n    x = 'true_ages',\n    y = 'Total',\n    title = 'Total Occurences of \"Brittany\" by Age',\n    labels = {'true_ages' : 'Age', 'Total' : 'Total per age group'}\n)\n\nbrit_fig.update_layout(#formatting title\n    title_x = 0.5\n)\n\nbrit_fig.show()\n\n\n                                                \n\n\nThis graph is useful to help visualize the data; however, the real value lies in some statistical analysis of the data, using the .describe() method:\n\n\nShow the code\n#brittany_df['true_ages'].describe()\n#reference for analysis\n\nbrit_fig.add_vline(x = 20.75, line_dash = 'dot', line_color = 'red')\nbrit_fig.add_vline(x = 44.25, line_dash = 'dot', line_color = 'red')\n\n#displays lines at 25% and 75% quartiles\n\nbrit_fig.show()\n\n\n                                                \n\n\n\n\nAnalysis\n50% of the data falls between the ages of 20.75 and 44.25, with a mean age of 32.5. Thus, it can reasonably be assumed that the if one were to encounter a person named “Brittany,” it is reasonably probable that they are between the ages of 20 and 45, most likely around the age of 32. The range of the age data is [9, 56], so I would assume that the person I am talking to is not younger than 9 or older than 56.",
    "crumbs": [
      "DS250 Projects",
      "Project 1"
    ]
  },
  {
    "objectID": "Projects/proj.1.writeup.1.19.23.html#questiontask-3",
    "href": "Projects/proj.1.writeup.1.19.23.html#questiontask-3",
    "title": "Client Report - Project 1: Names",
    "section": "Question|Task 3",
    "text": "Question|Task 3\nMary, Martha, Peter, and Paul are all Christian names. From 1920 - 2000, compare the name usage of each of the four names. What trends do you notice?\n\nWorking with the Data\nFirst, I pulled the data and created a visualization based on the given parameters.\n\n\nShow the code\nmmpp_df = df.query('name == \"Mary\" or name == \"Martha\" or\\\n                name == \"Peter\" or name == \"Paul\"').query('1920 &lt;= year &lt;= 2000')#creates new df\n\nmmpp_fig = px.line(#creates line plot for year vs Total\n    mmpp_df,\n    x = 'year',\n    y = 'Total',\n    color = 'name',\n    title = 'Historical Use of \"Martha,\" \"Mary,\" \"Paul,\" and \"Peter\"',\n    labels = {'year': 'Year'}\n)\n\nmmpp_fig.update_layout(\n    title_x = 0.5\n)\n\n#creates lines that contain the peak of each line\nmmpp_fig.add_vline(x = 1943, line_dash = 'dot', line_color = 'black')\nmmpp_fig.add_vline(x = 1960, line_dash = 'dot', line_color = 'black')\n\nmmpp_fig.show()\n\n\n                                                \n\n\n\n\nAnalysis\nAs shown in the graph, all four names have similar popularity intervals, with each name’s peak popularity occuring between 1947 and 1956. Mary was significantly more popular than the others, with a peak total (53.791k) twice as great as the next most popular name, Paul (25.6625k). All four names have seen significant decline in popularity in recent years. I would speculate that these names have declined in popularity because the Bible has also declined in popularity.",
    "crumbs": [
      "DS250 Projects",
      "Project 1"
    ]
  },
  {
    "objectID": "Projects/proj.1.writeup.1.19.23.html#questiontask-4",
    "href": "Projects/proj.1.writeup.1.19.23.html#questiontask-4",
    "title": "Client Report - Project 1: Names",
    "section": "Question|Task 4",
    "text": "Question|Task 4\nThink of a unique name from a famous movie. Plot the usage of that name and see how changes line up with the movie release. Does it look like the movie had an effect on usage?\n\nWorking with the Data\nFor this task, I decided to use the character Marty McFly from “Back to the Future.” The first movie was released in 1985. I reasoned that many parents who look to name their children Marty might legally name their child Martin and call them Marty as a nickname. Thus, both names are included in my analysis.\nHere is the visualization I created; the black line indicates the movie’s release year, and the green line shows a small spike following the release:\n\n\nShow the code\n#creates new df\ndfa = df.query('name == \"Marty\" or name == \"Martin\"')\n\n#creates visualization with year vs Total\ndfa_fig = px.line(\n    dfa,\n    x = 'year',\n    y = 'Total',\n    color = 'name',\n    title = 'Historical Use of \"Martin\" and \"Marty\"',\n    labels = {'year': 'Year'}\n)\ndfa_fig.update_layout(\n    title_x = 0.5\n)\n\ndfa_fig.add_vline(x = 1985, \n                  line_dash = 'dot',\n                  line_color = 'black',\n                  name = 'Release of Back to the Future'\n                  )#release year of movie\ndfa_fig.add_vline(x = 1990, \n                  line_dash = 'dot', \n                  line_color = 'green',\n                  name = 'Increase in \"Martin\" in 1990'\n                  )#small spike in total\ndfa_fig.show()\n\n\n                                                \n\n\n\n\nAnalysis\nBoth names seem to share similar trends in popularity; thus I feel it was right to include both in my analysis. Although Martin’s spike was more pronounced, both names experienced a small increase in popularity in 1990. My theory for this is that the majority of fans of the movie “Back to the Future” were teenagers at the time of the release, and it was several years after that many of them started to have children.\nHowever, I noticed that the names were significantly more popular in the interval from the 1950’s to 1968, and I was curious as to why. After some speculation, I have developed another theory for this. Below, the vertical lines show the bounds of the interval during which Martin Luther King Jr. was heavily involved in the Civil Rights Movement:\n\n\nShow the code\ndfa_fig = px.line(\n    dfa,\n    x = 'year',\n    y = 'Total',\n    color = 'name',\n    title = 'Historical Use of \"Martin\" and \"Marty\"',\n    labels = {'year': 'Year'}\n)\ndfa_fig.update_layout(\n    title_x = 0.5\n)\n\ndfa_fig.add_vline(x = 1955, \n                  line_dash = 'dot', \n                  line_color = 'maroon'\n                )\ndfa_fig.add_vline(x = 1968, \n                  line_dash = 'dot', \n                  line_color = 'maroon'\n                )\n\ndfa_fig.show()\n\n\n                                                \n\n\nBased on this observation, I hypothesize that world events and real people have a much bigger impact on the names of newborn babies than fictional characters do.",
    "crumbs": [
      "DS250 Projects",
      "Project 1"
    ]
  },
  {
    "objectID": "Projects/proj.3.writeup.2.22.24.html",
    "href": "Projects/proj.3.writeup.2.22.24.html",
    "title": "Client Report - Project 3: Finding relationships in baseball.",
    "section": "",
    "text": "Show the code\n#establishes file connection\ncon = sqlite3.connect(r'C:\\Users\\andre\\OneDrive\\Desktop\\ds250\\data_files\\lahmansbaseballdb.sqlite')",
    "crumbs": [
      "DS250 Projects",
      "Project 3"
    ]
  },
  {
    "objectID": "Projects/proj.3.writeup.2.22.24.html#elevator-pitch",
    "href": "Projects/proj.3.writeup.2.22.24.html#elevator-pitch",
    "title": "Client Report - Project 3: Finding relationships in baseball.",
    "section": "Elevator pitch",
    "text": "Elevator pitch\nBelow you will find results for the data that was requested. This includes information about players that attended BYU-Idaho, as well as information relating to batting averages for many players with more than one, then more than ten at-bats. You will find a list of career batting averages for players with more than 100 at-bats. Lastly, you will find an analysis and a comparison of win rates between the Dodgers and the Giants.",
    "crumbs": [
      "DS250 Projects",
      "Project 3"
    ]
  },
  {
    "objectID": "Projects/proj.3.writeup.2.22.24.html#questiontask-1",
    "href": "Projects/proj.3.writeup.2.22.24.html#questiontask-1",
    "title": "Client Report - Project 3: Finding relationships in baseball.",
    "section": "Question|Task 1",
    "text": "Question|Task 1\nWrite an SQL query to create a new dataframe about baseball players who attended BYU-Idaho. The new table should contain five columns: playerID, schoolID, salary, and the yearID/teamID associated with each salary. Order the table by salary (highest to lowest) and print out the table in your report.\nThis task requires just an SQL query to retrieve the requested information:\n\n\nShow the code\n# selects the above columns; all come from salaries except school id\n#school id comes from collegeplaying; salaries joined on college playing by playerID\n#where school is byui\n#ordered by salary, highest to lowest\n\nid_sql = '''\nSELECT DISTINCT\n\nsalaries.playerID AS 'Player ID',\n(nameFirst || ' ' || nameLast) AS 'Player Name',\nsalaries.salary AS Salary, \nsalaries.yearID AS Year, \nsalaries.teamID AS Team,\nteams.name AS 'Team Name',\ncollegeplaying.schoolID AS 'School ID'\n\nFROM \nsalaries \nJOIN \npeople ON salaries.playerID = people.playerID\nJOIN \ncollegeplaying ON salaries.playerID = collegeplaying.playerID\nJOIN \nteams on teams.teamID = salaries.teamID AND teams.yearID = salaries.yearID\n\n\nWHERE collegeplaying.schoolID = 'idbyuid'\n\nORDER BY salary DESC;\n\n'''\n\ndf = pd.read_sql_query(id_sql, con)\nformat_df = df.copy()\nformat_df['Salary'] = format_df['Salary'].apply(\n    lambda x: '${:,.0f}'.format(x)\n)\n\n\n\nResults\n\n\nShow the code\nformat_df\n\n\n\n\n\n\n\n\n\n\nPlayer ID\nPlayer Name\nSalary\nYear\nTeam\nTeam Name\nSchool ID\n\n\n\n\n0\nlindsma01\nMatt Lindstrom\n$4,000,000\n2014\nCHA\nChicago White Sox\nidbyuid\n\n\n1\nlindsma01\nMatt Lindstrom\n$3,600,000\n2012\nBAL\nBaltimore Orioles\nidbyuid\n\n\n2\nlindsma01\nMatt Lindstrom\n$2,800,000\n2011\nCOL\nColorado Rockies\nidbyuid\n\n\n3\nlindsma01\nMatt Lindstrom\n$2,300,000\n2013\nCHA\nChicago White Sox\nidbyuid\n\n\n4\nlindsma01\nMatt Lindstrom\n$1,625,000\n2010\nHOU\nHouston Astros\nidbyuid\n\n\n5\nstephga01\nGarrett Stephenson\n$1,025,000\n2001\nSLN\nSt. Louis Cardinals\nidbyuid\n\n\n6\nstephga01\nGarrett Stephenson\n$900,000\n2002\nSLN\nSt. Louis Cardinals\nidbyuid\n\n\n7\nstephga01\nGarrett Stephenson\n$800,000\n2003\nSLN\nSt. Louis Cardinals\nidbyuid\n\n\n8\nstephga01\nGarrett Stephenson\n$550,000\n2000\nSLN\nSt. Louis Cardinals\nidbyuid\n\n\n9\nlindsma01\nMatt Lindstrom\n$410,000\n2009\nFLO\nFlorida Marlins\nidbyuid\n\n\n10\nlindsma01\nMatt Lindstrom\n$395,000\n2008\nFLO\nFlorida Marlins\nidbyuid\n\n\n11\nlindsma01\nMatt Lindstrom\n$380,000\n2007\nFLO\nFlorida Marlins\nidbyuid\n\n\n12\nstephga01\nGarrett Stephenson\n$215,000\n1999\nSLN\nSt. Louis Cardinals\nidbyuid\n\n\n13\nstephga01\nGarrett Stephenson\n$185,000\n1998\nPHI\nPhiladelphia Phillies\nidbyuid\n\n\n14\nstephga01\nGarrett Stephenson\n$150,000\n1997\nPHI\nPhiladelphia Phillies\nidbyuid\n\n\n\n\n\n\n\n\nDisplayed above are the required columns, as well as some additional columns with some extra descriptive information.\nIt appears that two professional players have come from BYU-Idaho: Matt Lindstrom, and Garrett Stephenson. Garrett Stephenson mostly played for the Cardinals, and earned $1.025 million in his highest-earning year. Matt Lindstrom mostly played for the Marlins and the White Sox, and earned up to $4 million.",
    "crumbs": [
      "DS250 Projects",
      "Project 3"
    ]
  },
  {
    "objectID": "Projects/proj.3.writeup.2.22.24.html#questiontask-2",
    "href": "Projects/proj.3.writeup.2.22.24.html#questiontask-2",
    "title": "Client Report - Project 3: Finding relationships in baseball.",
    "section": "Question|Task 2",
    "text": "Question|Task 2\nThis three-part question requires you to calculate batting average (number of hits divided by the number of at-bats)\n\nTask A\n\nWrite an SQL query that provides playerID, yearID, and batting average for players with at least 1 at bat that year. Sort the table from highest batting average to lowest, and then by playerid alphabetically. Show the top 5 results in your report.\n\n\nResults\nDisplayed below are the expected results for the requested query.\n\n\nShow the code\nbat_sql = '''\nSELECT\nb.playerID AS 'Player ID',\n(p.nameFirst || ' ' || p.nameLast) AS 'Player Name',\nyearID AS 'Year ID',\nH as Hits, AB as 'At-bats',\n(CAST(H AS FLOAT) / AB) AS 'Batting Average'\nFROM batting b\nJOIN people p ON p.playerID = b.playerID\nWHERE AB &gt;= 1\nORDER BY (CAST(H AS FLOAT) / AB) DESC, b.playerID\nLIMIT 5;\n'''\n\nbat_df = pd.read_sql_query(bat_sql , con)\nbat_df\n\n\n\n\n\n\n\n\n\n\nPlayer ID\nPlayer Name\nYear ID\nHits\nAt-bats\nBatting Average\n\n\n\n\n0\naberal01\nAl Aber\n1957\n1\n1\n1.0\n\n\n1\nabernte02\nTed Abernathy\n1960\n1\n1\n1.0\n\n\n2\nabramge01\nGeorge Abrams\n1923\n1\n1\n1.0\n\n\n3\nacklefr01\nFritz Ackley\n1964\n1\n1\n1.0\n\n\n4\nalanirj01\nR. J. Alaniz\n2019\n1\n1\n1.0\n\n\n\n\n\n\n\n\nIt appears that the top results of this query contain players that have just 1 at-bat and just 1 hit, giving them a perfect batting average.\n\n\n\nTask B\n\nUse the same query as above, but only include players with at least 10 at bats that year. Print the top 5 results.\n\n\nResults\nBelow are the adjusted results.\n\n\nShow the code\nbat_sql = '''\nSELECT \nb.playerID AS 'Player ID',\n(p.nameFirst || ' ' || p.nameLast) AS 'Player Name',\nyearID AS 'Year ID',\nH as Hits, AB as 'At-bats',\n(CAST(H AS FLOAT) / AB) AS 'Batting Average'\nFROM batting b\nJOIN people p ON p.playerID = b.playerID\nWHERE AB &gt;= 10\nORDER BY (CAST(H AS FLOAT) / AB) DESC, b.playerID\nLIMIT 5;\n'''\n\nbat_df = pd.read_sql_query(bat_sql , con)\nbat_df\n\n\n\n\n\n\n\n\n\n\nPlayer ID\nPlayer Name\nYear ID\nHits\nAt-bats\nBatting Average\n\n\n\n\n0\nnymanny01\nNyls Nyman\n1974\n9\n14\n0.642857\n\n\n1\ncarsoma01\nMatt Carson\n2013\n7\n11\n0.636364\n\n\n2\naltizda01\nDave Altizer\n1910\n6\n10\n0.600000\n\n\n3\njohnsde01\nDeron Johnson\n1975\n6\n10\n0.600000\n\n\n4\nsilvech01\nCharlie Silvera\n1948\n8\n14\n0.571429\n\n\n\n\n\n\n\n\nAlthough these results will be more reflective of the players’ actual abilities, the Law of Large Numbers tells us we should be looking for higher numbers of at-bats than 10 to accurately represent the players’ abilities.\n\n\n\nTask C\n\nNow calculate the batting average for players over their entire careers (all years combined). Only include players with at least 100 at bats, and print the top 5 results.\n\n\nResults\nDisplayed are the results of the query.\n\n\nShow the code\ncareer_sql = '''\nSELECT \nb.playerID AS 'Player ID',\n(p.nameFirst || ' ' || p.nameLast) AS 'Player Name',\nMIN(b.yearID) AS 'Rookie Year',\nSUM(b.H) AS 'Career Hits',\nSUM(b.AB) AS 'Career At-Bats',\nCAST(SUM(b.H) AS FLOAT) / SUM(AB) AS 'Career Batting Average'\n\nFROM \nbatting b\nJOIN\npeople p ON p.playerID = b.playerID\n\nWHERE \nAB &gt;= 100\n\nGROUP BY \nb.playerID\n\nORDER BY \n'Career Batting Average' DESC\nLIMIT 5;\n'''\n\ncareer_df = pd.read_sql_query(career_sql, con)\ncareer_df\n\n\n\n\n\n\n\n\n\n\nPlayer ID\nPlayer Name\nRookie Year\nCareer Hits\nCareer At-Bats\nCareer Batting Average\n\n\n\n\n0\naaronha01\nHank Aaron\n1954\n3771\n12364\n0.304998\n\n\n1\naaronto01\nTommie Aaron\n1962\n173\n752\n0.230053\n\n\n2\nabbated01\nEd Abbaticchio\n1903\n728\n2852\n0.255259\n\n\n3\nabbeych01\nCharlie Abbey\n1893\n493\n1756\n0.280752\n\n\n4\nabbotfr01\nFred Abbott\n1903\n107\n513\n0.208577",
    "crumbs": [
      "DS250 Projects",
      "Project 3"
    ]
  },
  {
    "objectID": "Projects/proj.3.writeup.2.22.24.html#questiontask-3",
    "href": "Projects/proj.3.writeup.2.22.24.html#questiontask-3",
    "title": "Client Report - Project 3: Finding relationships in baseball.",
    "section": "Question|Task 3",
    "text": "Question|Task 3\nPick any two baseball teams and compare them using a metric of your choice (average salary, home runs, number of wins, etc). Write an SQL query to get the data you need, then make a graph using Plotly Express to visualize the comparison. What do you learn?\nI decided to compare the age-old rivalry between the Dodgers and the Giants. The point of a baseball game is to win; beyond this goal, not much else really matters more to the average fan. Being a Dodgers fan, I hope that the Dodgers come out on top with a higher win percentage. I decided to only use each team’s current name and city, because as a fan of the Los Angeles Dodgers, that is what matters most to me.\n\nResults\n\n\nShow the code\nwins_sql = '''\nSELECT\n\nname AS Name,\nsum(W) AS 'Total Wins',\nsum(G) AS 'Total Games',\n(CAST(sum(W) AS FLOAT) / sum(G)) AS 'Win Percentage'\n\nFROM teams\n\nWHERE name IN ('Los Angeles Dodgers' , 'San Francisco Giants')\n\nGROUP BY name\n\nORDER BY 'Win Percentage';\n'''\n\nwins_df = pd.read_sql_query(wins_sql, con)\n\n#applies visual formatting\nfor i in ['Total Wins', 'Total Games']:\n  wins_df[i] = wins_df[i].astype(str).apply(\n      lambda x: '{:,}'.format(int(x)))\n\nfig = px.histogram(\n  wins_df,\n  x = 'Name',\n  y = 'Win Percentage',\n  color = {'Los Angeles Dodgers': 'blue' , 'San Francisco Giants': 'red'},\n  labels = {'Name': 'Team' , 'Win Percentage': 'Win Percentage'},\n  title = 'Win Percentage By Team'\n).update_layout(\n  title_x = 0.5\n)\n\n#applies further formatting\nwins_df['Win Percentage'] = (wins_df['Win Percentage'] * 100).round(2).astype(str) + '%'\n\nfig.show()\n\n\n                                                \n\n\n\n\nShow the code\nwins_df\n\n\n\n\n\n\n\n\n\n\nName\nTotal Wins\nTotal Games\nWin Percentage\n\n\n\n\n0\nLos Angeles Dodgers\n5,350\n9,894\n54.07%\n\n\n1\nSan Francisco Giants\n5,098\n9,893\n51.53%\n\n\n\n\n\n\n\n\nThe current Dodgers have a better win percentage than the current Giants! My day is made.",
    "crumbs": [
      "DS250 Projects",
      "Project 3"
    ]
  },
  {
    "objectID": "Projects/proj.5.writeup.3.18.24.html",
    "href": "Projects/proj.5.writeup.3.18.24.html",
    "title": "Client Report - Project 5: The war with Star Wars",
    "section": "",
    "text": "I cleaned up the column names of the data set to allow for use in pandas. I manipulated and formatted the data to allow for use with a machine learning model, which included assigning integer values and one-hot-encoding other categorical variables. I established a target variable and features. I recreated charts from the article to verify a relationship between the article’s analysis and the data in GitHub. Then I trained and tested a model to predict whether a person’s household made over $50k a year or not based on the data provided. The model yielded about 58.57% accuracy.\n\n\nRead and format project data\n# Include and execute your code here\ndf = pd.read_csv(r'C:\\Users\\andre\\OneDrive\\Desktop\\ds250\\data_files\\starwars_edited.csv')\n\n#drops first row with descriptions of the columns from original data set\ndf = df.drop(index = 0)",
    "crumbs": [
      "DS250 Projects",
      "Project 5"
    ]
  },
  {
    "objectID": "Projects/proj.5.writeup.3.18.24.html#elevator-pitch",
    "href": "Projects/proj.5.writeup.3.18.24.html#elevator-pitch",
    "title": "Client Report - Project 5: The war with Star Wars",
    "section": "",
    "text": "I cleaned up the column names of the data set to allow for use in pandas. I manipulated and formatted the data to allow for use with a machine learning model, which included assigning integer values and one-hot-encoding other categorical variables. I established a target variable and features. I recreated charts from the article to verify a relationship between the article’s analysis and the data in GitHub. Then I trained and tested a model to predict whether a person’s household made over $50k a year or not based on the data provided. The model yielded about 58.57% accuracy.\n\n\nRead and format project data\n# Include and execute your code here\ndf = pd.read_csv(r'C:\\Users\\andre\\OneDrive\\Desktop\\ds250\\data_files\\starwars_edited.csv')\n\n#drops first row with descriptions of the columns from original data set\ndf = df.drop(index = 0)",
    "crumbs": [
      "DS250 Projects",
      "Project 5"
    ]
  },
  {
    "objectID": "Projects/proj.5.writeup.3.18.24.html#questiontask-1",
    "href": "Projects/proj.5.writeup.3.18.24.html#questiontask-1",
    "title": "Client Report - Project 5: The war with Star Wars",
    "section": "Question|Task 1",
    "text": "Question|Task 1\nShorten the column names and clean them up for easier use with pandas. Provide a table or list that exemplifies how you fixed the names.\nUnedited, pandas couldn’t even read the column headings. This is how the column headings read originally:\n\n\nShow the code\nwith open(r'C:\\Users\\andre\\OneDrive\\Desktop\\ds250\\data_files\\starwars.csv') as starwars:\n  txt = starwars.read()\ntxt = txt.split('\\n')\n\ntxt[0]\n\n\n'RespondentID,Have you seen any of the 6 films in the Star Wars franchise?,Do you consider yourself to be a fan of the Star Wars film franchise?,Which of the following Star Wars films have you seen? Please select all that apply.,,,,,,Please rank the Star Wars films in order of preference with 1 being your favorite film in the franchise and 6 being your least favorite film.,,,,,,\"Please state whether you view the following characters favorably, unfavorably, or are unfamiliar with him/her.\",,,,,,,,,,,,,,Which character shot first?,Are you familiar with the Expanded Universe?,Do you consider yourself to be a fan of the Expanded Universe?Œæ,Do you consider yourself to be a fan of the Star Trek franchise?,Gender,Age,Household Income,Education,Location (Census Region)'\n\n\nThere are many untitled columns, and many of the column names are far too long.\nI rewrote the CSV file so that the names were shorter, and every column had a proper heading. This is how the new headings appear:\n\n\nShow the code\ndf.columns\n\n\nIndex(['respondent_id', 'seen_any', 'fan', 'seen_ep1', 'seen_ep2', 'seen_ep3',\n       'seen_ep4', 'seen_ep5', 'seen_ep6', 'ranking_ep1', 'ranking_ep2',\n       'ranking_ep3', 'ranking_ep4', 'ranking_ep5', 'ranking_ep6',\n       'rank_hansolo', 'rank_lukeskywalker', 'rank_leiaorgana',\n       'rank_anakinskywalker', 'rank_obiwankenobi', 'rank_emperorpalpatine',\n       'rank_darthvader', 'rank_landocalrissian', 'rank_bobafett', 'rank_c3po',\n       'rank_r2d2', 'rank_jarjarbinks', 'rank_padmeamidala', 'rank_yoda',\n       'who_shot_first', 'know_expanded_universe', 'fan_of_expanded_universe',\n       'star_trek_fan', 'gender', 'age', 'household_income', 'education',\n       'location_census_region'],\n      dtype='object')\n\n\nThese headings should be a little bit more explicit and consistent, and they are in a form that pandas can read.",
    "crumbs": [
      "DS250 Projects",
      "Project 5"
    ]
  },
  {
    "objectID": "Projects/proj.5.writeup.3.18.24.html#questiontask-2",
    "href": "Projects/proj.5.writeup.3.18.24.html#questiontask-2",
    "title": "Client Report - Project 5: The war with Star Wars",
    "section": "Question|Task 2",
    "text": "Question|Task 2\n\nClean and format the data so that it can be used in a machine learning model. As you format the data, you should complete each item listed below. In your final report provide example(s) of the reformatted data with a short description of the changes made.\na. Filter the dataset to respondents that have seen at least one film.\nb. Create a new column that converts the age ranges to a single number. Drop the age range categorical column.\nc. Create a new column that converts the education groupings to a single number. Drop the school categorical column\nd. Create a new column that converts the income ranges to a single number. Drop the income range categorical column.\ne. Create your target (also known as “y” or “label”) column based on the new income range column.\nf. One-hot encode all remaining categorical columns.\n\n\nStep A\n\na. Filter the dataset to respondents that have seen at least one film.\n\n\n\nShow the code\n#drops na columns for haven't seen any movies\ndf = df.loc[~df[df.columns[3:9]].isna().all(axis=1)]\n\ndf.head(5)\n\n\n\n\n\n\n\n\n\n\nrespondent_id\nseen_any\nfan\nseen_ep1\nseen_ep2\nseen_ep3\nseen_ep4\nseen_ep5\nseen_ep6\nranking_ep1\nranking_ep2\nranking_ep3\nranking_ep4\nranking_ep5\nranking_ep6\nrank_hansolo\nrank_lukeskywalker\nrank_leiaorgana\nrank_anakinskywalker\nrank_obiwankenobi\nrank_emperorpalpatine\nrank_darthvader\nrank_landocalrissian\nrank_bobafett\nrank_c3po\nrank_r2d2\nrank_jarjarbinks\nrank_padmeamidala\nrank_yoda\nwho_shot_first\nknow_expanded_universe\nfan_of_expanded_universe\nstar_trek_fan\ngender\nage\nhousehold_income\neducation\nlocation_census_region\n\n\n\n\n1\n3.292880e+09\nYes\nYes\nStar Wars: Episode I The Phantom Menace\nStar Wars: Episode II Attack of the Clones\nStar Wars: Episode III Revenge of the Sith\nStar Wars: Episode IV A New Hope\nStar Wars: Episode V The Empire Strikes Back\nStar Wars: Episode VI Return of the Jedi\n3\n2\n1\n4\n5\n6\nVery favorably\nVery favorably\nVery favorably\nVery favorably\nVery favorably\nVery favorably\nVery favorably\nUnfamiliar (N/A)\nUnfamiliar (N/A)\nVery favorably\nVery favorably\nVery favorably\nVery favorably\nVery favorably\nI don't understand this question\nYes\nNo\nNo\nMale\n18-29\nNaN\nHigh school degree\nSouth Atlantic\n\n\n3\n3.292765e+09\nYes\nNo\nStar Wars: Episode I The Phantom Menace\nStar Wars: Episode II Attack of the Clones\nStar Wars: Episode III Revenge of the Sith\nNaN\nNaN\nNaN\n1\n2\n3\n4\n5\n6\nSomewhat favorably\nSomewhat favorably\nSomewhat favorably\nSomewhat favorably\nSomewhat favorably\nUnfamiliar (N/A)\nUnfamiliar (N/A)\nUnfamiliar (N/A)\nUnfamiliar (N/A)\nUnfamiliar (N/A)\nUnfamiliar (N/A)\nUnfamiliar (N/A)\nUnfamiliar (N/A)\nUnfamiliar (N/A)\nI don't understand this question\nNo\nNaN\nNo\nMale\n18-29\n$0 - $24,999\nHigh school degree\nWest North Central\n\n\n4\n3.292763e+09\nYes\nYes\nStar Wars: Episode I The Phantom Menace\nStar Wars: Episode II Attack of the Clones\nStar Wars: Episode III Revenge of the Sith\nStar Wars: Episode IV A New Hope\nStar Wars: Episode V The Empire Strikes Back\nStar Wars: Episode VI Return of the Jedi\n5\n6\n1\n2\n4\n3\nVery favorably\nVery favorably\nVery favorably\nVery favorably\nVery favorably\nSomewhat favorably\nVery favorably\nSomewhat favorably\nSomewhat unfavorably\nVery favorably\nVery favorably\nVery favorably\nVery favorably\nVery favorably\nI don't understand this question\nNo\nNaN\nYes\nMale\n18-29\n$100,000 - $149,999\nSome college or Associate degree\nWest North Central\n\n\n5\n3.292731e+09\nYes\nYes\nStar Wars: Episode I The Phantom Menace\nStar Wars: Episode II Attack of the Clones\nStar Wars: Episode III Revenge of the Sith\nStar Wars: Episode IV A New Hope\nStar Wars: Episode V The Empire Strikes Back\nStar Wars: Episode VI Return of the Jedi\n5\n4\n6\n2\n1\n3\nVery favorably\nSomewhat favorably\nSomewhat favorably\nSomewhat unfavorably\nVery favorably\nVery unfavorably\nSomewhat favorably\nNeither favorably nor unfavorably (neutral)\nVery favorably\nSomewhat favorably\nSomewhat favorably\nVery unfavorably\nSomewhat favorably\nSomewhat favorably\nGreedo\nYes\nNo\nNo\nMale\n18-29\n$100,000 - $149,999\nSome college or Associate degree\nWest North Central\n\n\n6\n3.292719e+09\nYes\nYes\nStar Wars: Episode I The Phantom Menace\nStar Wars: Episode II Attack of the Clones\nStar Wars: Episode III Revenge of the Sith\nStar Wars: Episode IV A New Hope\nStar Wars: Episode V The Empire Strikes Back\nStar Wars: Episode VI Return of the Jedi\n1\n4\n3\n6\n5\n2\nVery favorably\nVery favorably\nVery favorably\nVery favorably\nVery favorably\nNeither favorably nor unfavorably (neutral)\nVery favorably\nNeither favorably nor unfavorably (neutral)\nSomewhat favorably\nSomewhat favorably\nSomewhat favorably\nSomewhat favorably\nNeither favorably nor unfavorably (neutral)\nVery favorably\nHan\nYes\nNo\nYes\nMale\n18-29\n$25,000 - $49,999\nBachelor degree\nMiddle Atlantic\n\n\n\n\n\n\n\n\nThis chunk filters the data to only include respondents confirmed that they had seen one film.\n\n\nStep B\n\nb. Create a new column that converts the age ranges to a single number. Drop the age range categorical column.\n\n\n\nShow the code\n#new column for age ranges\n\n#looks at possible age values\ndf['age'].unique()\n\n#new method for replacing ages\nnew_ages = {\n  '18-29': 0,\n  '30-44': 1,\n  '45-60': 2,\n  '&gt; 60': 3\n}\n\n#makes changes\ndf['age'] = df['age'].replace(new_ages)\n\nprint('Unique values: ', df['age'].unique())\ndf['age'].head(5)\n\n\nUnique values:  [ 0.  1.  3.  2. nan]\n\n\n1    0.0\n3    0.0\n4    0.0\n5    0.0\n6    0.0\nName: age, dtype: float64\n\n\nThis code chunk sets each age range to be integer values. With 4 different categories, I used digits 0-3, assigning them incrementally according to chronological order of the ranges. For example, the youngest age range was given a value of 0, while the oldest age range was given a value of 3. It’s important to note that I decided not to scale these according to the data itself because the model I decided to go with in the end, RandomForestClassifier, doesn’t require scaling.\n\n\nStep C\n\nc. Create a new column that converts the education groupings to a single number. Drop the school categorical column\n\n\n\nShow the code\n#looks at unique education values\ndf['education'].unique()\n\n#dict of changes\nnew_ed = {\n  'Less than high school degree': 0,\n  'High school degree': 1,\n  'Some college or Associate degree': 2,\n  'Bachelor degree': 3,\n  'Graduate degree': 4\n}\n\n#apply changes\ndf['education'] = df['education'].replace(new_ed)\n\nprint('Unique values: ', df['education'].unique())\ndf['education'].head(5)\n\n\nUnique values:  [ 1.  2.  3.  4. nan  0.]\n\n\n1    1.0\n3    1.0\n4    2.0\n5    2.0\n6    3.0\nName: education, dtype: float64\n\n\nThis code chunk assigns integer values for the unique possible values in the 'education' column. With 5 unique values, integers 0-4 were used. Values were assigned incrementally respective to the level of education. For example, ‘less than high school degree’ was given a value of 0, while ‘Graduate degree’ was given a value of 4.\n\n\nStep D\n\nd. Create a new column that converts the income ranges to a single number. Drop the income range categorical column.\n\n\n\nShow the code\n#looks at values in column\ndf['household_income'].unique()\n\n#dict method for changes\nnew_inc = {\n  '$0 - $24,999': 0,\n  '$25,000 - $49,999': 1,\n  '$50,000 - $99,999': 2,\n  '$100,000 - $149,999': 3,\n  '$150,000+': 4\n}\n\n#applies change\ndf['household_income'] = df['household_income'].replace(new_inc)\n\nprint('Unique values: ' , df['household_income'].unique())\ndf['household_income'].head(5)\n\n\nUnique values:  [nan  0.  3.  1.  2.  4.]\n\n\n1    NaN\n3    0.0\n4    3.0\n5    3.0\n6    1.0\nName: household_income, dtype: float64\n\n\nWith 5 different income ranges, I assigned integer values 0-4 to the ranges with respect to their magnitudes, as above with the previous two columns.\n\n\nStep E\n\ne. Create your target (also known as “y” or “label”) column based on the new income range column.\n\nThe target should indicate whether the recorded person makes over $50k a year or not. Thus, I will use the 'household_income' column to create the target.\n\n\nShow the code\n# 1 is yes, 0 is no\n\ndf['over50k'] = np.where(\n  df['household_income'] &gt; 1,\n  1,\n  0\n)\n\ndf['over50k'].head(5)\n\n\n1    0\n3    0\n4    1\n5    1\n6    0\nName: over50k, dtype: int32\n\n\nRecords where the household_income was over $50K were assigned a value of 1 to indicate True, and records where the household_income was under $50k were assigned a value of 0, to indicate False.\n\n\nStep F\n\nf. One-hot encode all remaining categorical columns.\n\nThe method I chose for this task was to use pd.get_dummies().\n\n\nShow the code\n#creates features\nfeatures = df.drop([\n  'seen_any', #only yes in this filtered df, irrelevant\n  'respondent_id',\n  'over50k',\n  'household_income'\n], axis = 1)\n\n#creates target\ntarget = df['over50k']\n\n# properly associates positive and negative responses for making better sense of encoded column names\nfor i in features.columns[1:7]:\n  features[i] = np.where(features[i].isna() , 'no' , 'yes')\n\n# columns to one-hot encode\nenc_data = [\n'fan',\n'seen_ep1', \n'seen_ep2', \n'seen_ep3', \n'seen_ep4', \n'seen_ep5', \n'seen_ep6', \n'rank_hansolo', \n'rank_lukeskywalker', \n'rank_leiaorgana', \n'rank_anakinskywalker', \n'rank_obiwankenobi', \n'rank_emperorpalpatine', \n'rank_darthvader',\n'rank_landocalrissian', \n'rank_bobafett', \n'rank_c3po', \n'rank_r2d2',\n'rank_jarjarbinks', \n'rank_padmeamidala', \n'rank_yoda', \n'who_shot_first',\n'know_expanded_universe', \n'fan_of_expanded_universe', \n'star_trek_fan',\n'gender',\n'location_census_region'\n]\n\n# gets dummies for features; target doesn't need to be encoded\nonehot_feat = pd.get_dummies(features , columns = enc_data, prefix = enc_data)\n\n#renames columns\nonehot_feat.columns = onehot_feat.columns.str.replace(' ' , '_')\n\n#Changes from boolean to int\nfor col in onehot_feat.columns[9:]:\n  onehot_feat[col] = onehot_feat[col].astype(int)\n\nfor col in onehot_feat.columns:\n  onehot_feat[col] = onehot_feat[col].astype(float)\n  mean = onehot_feat[col].mean()\n  #print(col , mean)\n  onehot_feat[col] = onehot_feat[col].fillna(mean)\n\ntarget = target.astype(float)\n\n#splits data for testing\ntrain_input, test_input, train_targets, test_targets = train_test_split(onehot_feat, target, test_size=.3, random_state = 42)\n\nonehot_feat.head(5)\n\n\n\n\n\n\n\n\n\n\nranking_ep1\nranking_ep2\nranking_ep3\nranking_ep4\nranking_ep5\nranking_ep6\nage\neducation\nfan_No\nfan_Yes\nseen_ep1_no\nseen_ep1_yes\nseen_ep2_no\nseen_ep2_yes\nseen_ep3_no\nseen_ep3_yes\nseen_ep4_no\nseen_ep4_yes\nseen_ep5_no\nseen_ep5_yes\nseen_ep6_no\nseen_ep6_yes\nrank_hansolo_Neither_favorably_nor_unfavorably_(neutral)\nrank_hansolo_Somewhat_favorably\nrank_hansolo_Somewhat_unfavorably\nrank_hansolo_Unfamiliar_(N/A)\nrank_hansolo_Very_favorably\nrank_hansolo_Very_unfavorably\nrank_lukeskywalker_Neither_favorably_nor_unfavorably_(neutral)\nrank_lukeskywalker_Somewhat_favorably\nrank_lukeskywalker_Somewhat_unfavorably\nrank_lukeskywalker_Unfamiliar_(N/A)\nrank_lukeskywalker_Very_favorably\nrank_lukeskywalker_Very_unfavorably\nrank_leiaorgana_Neither_favorably_nor_unfavorably_(neutral)\nrank_leiaorgana_Somewhat_favorably\nrank_leiaorgana_Somewhat_unfavorably\nrank_leiaorgana_Unfamiliar_(N/A)\nrank_leiaorgana_Very_favorably\nrank_leiaorgana_Very_unfavorably\nrank_anakinskywalker_Neither_favorably_nor_unfavorably_(neutral)\nrank_anakinskywalker_Somewhat_favorably\nrank_anakinskywalker_Somewhat_unfavorably\nrank_anakinskywalker_Unfamiliar_(N/A)\nrank_anakinskywalker_Very_favorably\nrank_anakinskywalker_Very_unfavorably\nrank_obiwankenobi_Neither_favorably_nor_unfavorably_(neutral)\nrank_obiwankenobi_Somewhat_favorably\nrank_obiwankenobi_Somewhat_unfavorably\nrank_obiwankenobi_Unfamiliar_(N/A)\nrank_obiwankenobi_Very_favorably\nrank_obiwankenobi_Very_unfavorably\nrank_emperorpalpatine_Neither_favorably_nor_unfavorably_(neutral)\nrank_emperorpalpatine_Somewhat_favorably\nrank_emperorpalpatine_Somewhat_unfavorably\nrank_emperorpalpatine_Unfamiliar_(N/A)\nrank_emperorpalpatine_Very_favorably\nrank_emperorpalpatine_Very_unfavorably\nrank_darthvader_Neither_favorably_nor_unfavorably_(neutral)\nrank_darthvader_Somewhat_favorably\nrank_darthvader_Somewhat_unfavorably\nrank_darthvader_Unfamiliar_(N/A)\nrank_darthvader_Very_favorably\nrank_darthvader_Very_unfavorably\nrank_landocalrissian_Neither_favorably_nor_unfavorably_(neutral)\nrank_landocalrissian_Somewhat_favorably\nrank_landocalrissian_Somewhat_unfavorably\nrank_landocalrissian_Unfamiliar_(N/A)\nrank_landocalrissian_Very_favorably\nrank_landocalrissian_Very_unfavorably\nrank_bobafett_Neither_favorably_nor_unfavorably_(neutral)\nrank_bobafett_Somewhat_favorably\nrank_bobafett_Somewhat_unfavorably\nrank_bobafett_Unfamiliar_(N/A)\nrank_bobafett_Very_favorably\nrank_bobafett_Very_unfavorably\nrank_c3po_Neither_favorably_nor_unfavorably_(neutral)\nrank_c3po_Somewhat_favorably\nrank_c3po_Somewhat_unfavorably\nrank_c3po_Unfamiliar_(N/A)\nrank_c3po_Very_favorably\nrank_c3po_Very_unfavorably\nrank_r2d2_Neither_favorably_nor_unfavorably_(neutral)\nrank_r2d2_Somewhat_favorably\nrank_r2d2_Somewhat_unfavorably\nrank_r2d2_Unfamiliar_(N/A)\nrank_r2d2_Very_favorably\nrank_r2d2_Very_unfavorably\nrank_jarjarbinks_Neither_favorably_nor_unfavorably_(neutral)\nrank_jarjarbinks_Somewhat_favorably\nrank_jarjarbinks_Somewhat_unfavorably\nrank_jarjarbinks_Unfamiliar_(N/A)\nrank_jarjarbinks_Very_favorably\nrank_jarjarbinks_Very_unfavorably\nrank_padmeamidala_Neither_favorably_nor_unfavorably_(neutral)\nrank_padmeamidala_Somewhat_favorably\nrank_padmeamidala_Somewhat_unfavorably\nrank_padmeamidala_Unfamiliar_(N/A)\nrank_padmeamidala_Very_favorably\nrank_padmeamidala_Very_unfavorably\nrank_yoda_Neither_favorably_nor_unfavorably_(neutral)\nrank_yoda_Somewhat_favorably\nrank_yoda_Somewhat_unfavorably\nrank_yoda_Unfamiliar_(N/A)\nrank_yoda_Very_favorably\nrank_yoda_Very_unfavorably\nwho_shot_first_Greedo\nwho_shot_first_Han\nwho_shot_first_I_don't_understand_this_question\nknow_expanded_universe_No\nknow_expanded_universe_Yes\nfan_of_expanded_universe_No\nfan_of_expanded_universe_Yes\nstar_trek_fan_No\nstar_trek_fan_Yes\ngender_Female\ngender_Male\nlocation_census_region_East_North_Central\nlocation_census_region_East_South_Central\nlocation_census_region_Middle_Atlantic\nlocation_census_region_Mountain\nlocation_census_region_New_England\nlocation_census_region_Pacific\nlocation_census_region_South_Atlantic\nlocation_census_region_West_North_Central\nlocation_census_region_West_South_Central\n\n\n\n\n1\n3.0\n2.0\n1.0\n4.0\n5.0\n6.0\n0.0\n1.0\n0.0\n1.0\n0.0\n1.0\n0.0\n1.0\n0.0\n1.0\n0.0\n1.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n1.0\n0.0\n1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n\n\n3\n1.0\n2.0\n3.0\n4.0\n5.0\n6.0\n0.0\n1.0\n1.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n1.0\n1.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n\n\n4\n5.0\n6.0\n1.0\n2.0\n4.0\n3.0\n0.0\n2.0\n0.0\n1.0\n0.0\n1.0\n0.0\n1.0\n0.0\n1.0\n0.0\n1.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n\n\n5\n5.0\n4.0\n6.0\n2.0\n1.0\n3.0\n0.0\n2.0\n0.0\n1.0\n0.0\n1.0\n0.0\n1.0\n0.0\n1.0\n0.0\n1.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n1.0\n0.0\n1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n\n\n6\n1.0\n4.0\n3.0\n6.0\n5.0\n2.0\n0.0\n3.0\n0.0\n1.0\n0.0\n1.0\n0.0\n1.0\n0.0\n1.0\n0.0\n1.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n1.0\n1.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n\n\n\n\n\n\nFirst, I split the data into features and the target. Then I made changes to the values of features to allow for easier encoding, and I made a list of columns that I wanted to get dummies for. I applied the get_dummies() function, then cleaned up the new column names for the algorithm. To also allow the algorithm to read the data better, I changed the boolean values to 1 and 0, and I filled null values with the mean of the respective columns. Finally, I chose this moment to split the data into training and testing sets. A ratio of 70/30 training to testing data yielded the best results.",
    "crumbs": [
      "DS250 Projects",
      "Project 5"
    ]
  },
  {
    "objectID": "Projects/proj.5.writeup.3.18.24.html#questiontask-3",
    "href": "Projects/proj.5.writeup.3.18.24.html#questiontask-3",
    "title": "Client Report - Project 5: The war with Star Wars",
    "section": "Question|Task 3",
    "text": "Question|Task 3\nValidate that the data provided on GitHub lines up with the article by recreating 2 of the visuals from the article.\nThe first chart I chose to recreate was the first chart in the article that describes the proportion of respondents that have seen each film out of the respondents that saw any film.\n\n\nShow the code\nseen_eps = features[features.columns[1:7]]\n\nseen_eps = seen_eps.replace('yes' , 1)\nseen_eps = seen_eps.replace('no' , 0)\n\n\nfig = px.bar(\n  y = seen_eps.columns[::-1],\n  x = (seen_eps.mean()*100).round()[::-1],\n  orientation = 'h',\n  title = 'Which Star Wars Movies Have You Seen?'\n).update_layout(\n  title_x = 0.5,\n  yaxis_title = 'Episodes Seen',\n  xaxis_title = 'Percent that have Seen Episode'\n)\n\nfig.show()\n\n\n                                                \n\n\nThe proportions and values for each episode correspond to those found in the chart on the website. The second chart I decided to recreate was the last one titled “Who Shot First?”\n\n\nShow the code\nwhoshot = features['who_shot_first']\n\ncounts = (whoshot.value_counts() / len(whoshot) *100).reset_index()\n\nfig2 = px.histogram(\n  y = [counts['who_shot_first'][1] , counts['who_shot_first'][2] , counts['who_shot_first'][0]],\n  x = [counts['count'][1] , counts['count'][2] , counts['count'][0]],\n  orientation = 'h',\n  title = 'Who Shot First?'\n).update_layout(\n  title_x = 0.5,\n  xaxis_title = 'Percent of Votes',\n  yaxis_title = 'Character'\n)\n\nfig2.show()\n\n\n                                                \n\n\nThis chart’s proportions and values also match those found in the chart in the analysis on the website. Given that these two charts match up very well to the charts found on the website, I conclude that the data provided on GitHub corresponds to the analysis provided in the article.",
    "crumbs": [
      "DS250 Projects",
      "Project 5"
    ]
  },
  {
    "objectID": "Projects/proj.5.writeup.3.18.24.html#questiontask-4",
    "href": "Projects/proj.5.writeup.3.18.24.html#questiontask-4",
    "title": "Client Report - Project 5: The war with Star Wars",
    "section": "Question|Task 4",
    "text": "Question|Task 4\nBuild a machine learning model that predicts whether a person makes more than $50k. Describe your model and report the accuracy.\nI tried several different models, and none performed very well. I was unable to train a model to perform at a higher accuracy than about 58.57%. Objectively, this isn’t very good. In theory, a pure guess would yield around 50% accuracy. Ultimately, I decided to go with RandomForestClassifier, because that model had the highest accuracy of all the models I tried.\n\nRandomSearch for RandomForestClassifier\nFirst I hypertuned the parameters using RandomSearchCV, which randomly searched for the optimal parameters within the parameter grid that I passed to it.\n\n\nShow the code\n#performs random search\n\nparam_grid = {\n    'n_estimators': randint(100, 1000, 100),\n    'criterion': ['gini', 'entropy'],\n    'max_depth': randint(3, 20, 100), \n    'min_samples_split': randint(2, 20, 100),\n    'min_samples_leaf': randint(1, 20, 100), \n    'max_features': ['auto', 'sqrt', 'log2'], \n    'bootstrap': [True, False],\n    'random_state': [42]\n}\n\n# Create the RandomizedSearchCV\nrandom_search = RandomizedSearchCV(\n    RandomForestClassifier(),\n    param_distributions=param_grid,\n    n_iter=50,\n    scoring='accuracy',\n    n_jobs=-1,\n    cv=5,\n    verbose=1,\n    random_state=42\n)\n\nrandom_search.fit(train_input, train_targets)\n\nbest_params = random_search.best_params_\n\nprint('Best Parameters: ', best_params)\n\n\n\n\nClassifier Train and Predict\nUsing these hyperparameters, I went ahead and trained the model on the training input and the training targets. Then, I had the model predict the testing targets based on the testing data. I measured the model’s performance using several different metrics.\n\n\nShow the code\nclassifier = None #resets variable to rerun chunk\n\nclassifier = RandomForestClassifier(\n  bootstrap = False,\n  criterion = 'gini',\n  max_depth = 111,\n  max_features = 'sqrt',\n  min_samples_leaf = 105,\n  min_samples_split = 102,\n  n_estimators = 1018,\n  random_state = 42\n)\n\nclassifier.fit(train_input , train_targets)\n\n#predict\ntargets_predicted = classifier.predict(test_input)\n\nnp1 = test_targets.to_numpy()\nnp2 = targets_predicted\n\n#Calculates Evaluation Metrics\nacc = round(metrics.accuracy_score(np1 , np2) * 100, 2)\nprec = round(metrics.precision_score(np1, np2) * 100, 2)\nrec = round(metrics.recall_score(np1, np2) * 100, 2)\nf1 = round(metrics.f1_score(np1, np2) * 100, 2)\n\nprint('Accuracy:' , str(acc) + '%')\nprint('Precision:' , str(prec) + '%')\nprint('Recall:' , str(rec) + '%')\nprint('f1:' , str(f1) + '%')\n\n\nAccuracy: 58.57%\nPrecision: 55.91%\nRecall: 82.54%\nf1: 66.67%\n\n\n\n\nConclusion\nThe model performs at about 58.57% accuracy, with an f1 score of about 66.67%. In my opinion, this is not very good, but I do think it is understandable. There isn’t a very strong relationship between Star Wars opinions and income. Below is a correlation matrix showing the correlation between different variables in the data and the target column 'over50k'.\n\n\nShow the code\ncordat = onehot_feat\n\ncordat['over50k'] = df['over50k']\n\ncorrelation_matrix = cordat.corr()\n# Get absolute values of correlation matrix\nabs_corr = correlation_matrix.abs()\n\n# Get the indices of the top n correlation values\nn = 11  # Adjust as needed\ntop_corr_cols = abs_corr.nlargest(n, 'over50k')['over50k'].index\n\n# Print top n correlation values\nprint(correlation_matrix.loc[top_corr_cols[1:], 'over50k'])\n\n\neducation                                                            0.164724\nrank_r2d2_Neither_favorably_nor_unfavorably_(neutral)               -0.107875\nrank_leiaorgana_Very_favorably                                       0.097353\nage                                                                  0.096485\nranking_ep2                                                          0.092864\nrank_leiaorgana_Neither_favorably_nor_unfavorably_(neutral)         -0.089756\nrank_emperorpalpatine_Neither_favorably_nor_unfavorably_(neutral)    0.089667\nrank_obiwankenobi_Very_favorably                                     0.087745\nrank_lukeskywalker_Very_favorably                                    0.087260\nrank_yoda_Neither_favorably_nor_unfavorably_(neutral)               -0.082688\nName: over50k, dtype: float64\n\n\nNo variable seems to have a higher absolute correlation coefficient value than education, which is about 0.16. The majority of variables have significantly lower correlation. This indicates the data is very weakly correlated to the target, bordering on not correlated at all. This brings into question how relevant this data realistically is for predicting household income. In my opinion, it isn’t very useful for that purpose. One thing to try for the future is to avoid one-hot-encoding for use with tree algorithms, per the advice of the article provided in the readings for this project about pd.get_dummies().",
    "crumbs": [
      "DS250 Projects",
      "Project 5"
    ]
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Andrew Belz’s Resume",
    "section": "",
    "text": "Student at BYU-Idaho"
  },
  {
    "objectID": "resume.html#education",
    "href": "resume.html#education",
    "title": "Andrew Belz’s Resume",
    "section": "Education",
    "text": "Education\nExpected 2027 Brigham Young University - Idaho, Rexburg, ID\n\nMajor: Data Science\n4.0 Major GPA"
  },
  {
    "objectID": "resume.html#service-and-work-history",
    "href": "resume.html#service-and-work-history",
    "title": "Andrew Belz’s Resume",
    "section": "Service and Work History",
    "text": "Service and Work History\n2022-2023 Solar Technician, MSI Solar, Bakersfield, CA\n\nSystem Design: Designed and tested photovoltaic system schemas to optimize efficiency and maximize space constraints.\nData Monitoring and Analysis: Implented and utilized software tools to monitor, analyze, and troubleshoot the performance of installed solar systems.\nTeam and Customer Collaboration: Coordinated and collaborated with customers, electricians, technicians, and contractors to deliver customer-involved solutions and optimize operational efficiency."
  },
  {
    "objectID": "Templates/DS250_Template.html",
    "href": "Templates/DS250_Template.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Uncomment the entire section to use this template\n\n\n\n\n\n\n\n Back to top"
  }
]